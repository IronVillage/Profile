{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc449fc7-b62b-4d1e-8d76-f69abac67470",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52372a08-a754-48eb-8f06-75dc7f1b878c",
   "metadata": {},
   "source": [
    "I was unsure exactly how to format this. I didn't want to make it a blog article or Kaggle post, since that is not really me. I'll take you through my process for approaching this problem pretty much as it unfolded. \n",
    "\n",
    "Modeling is an iterative process, and for me anyway, can be messy. I did not want to sanitize the notebook too much, since I felt the whole point of this was to gain an understanding of how I solve problems.\n",
    "\n",
    "Testing and exploration overtime is the key for me. The goal is always to build a strong intuitive understanding of the data and the sport, then go from there. \n",
    "\n",
    "I tried to get things done in about a week. So this wouldsay this is not a \"Final\" model, but I think it's a great place to start. I make some comments and annotations along the way, but as you will see, presenting my work is new for me, but this was a fun project none the less. Very excited to talk about this when we meet.\n",
    "\n",
    "I'll try and not give you too much tedious comments to read. I figure you guys know most of this.  :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bfac48-95ad-4a5b-9812-2c2af39e3035",
   "metadata": {},
   "source": [
    "# Loading, Cleaning and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a46776be-12f8-4b03-8e55-0df01589a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "503f906b-9ff2-40b4-81e3-80d4e918ef18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tournament_id</th>\n",
       "      <th>tournament_name</th>\n",
       "      <th>region</th>\n",
       "      <th>tier</th>\n",
       "      <th>date_start</th>\n",
       "      <th>match_id</th>\n",
       "      <th>map_id</th>\n",
       "      <th>map_order</th>\n",
       "      <th>prematch_home</th>\n",
       "      <th>home_team_id</th>\n",
       "      <th>away_team_id</th>\n",
       "      <th>map_win_team_id</th>\n",
       "      <th>results_away_kills</th>\n",
       "      <th>results_home_kills</th>\n",
       "      <th>results_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5649</td>\n",
       "      <td>WSL Season 8</td>\n",
       "      <td>asia</td>\n",
       "      <td>tier2</td>\n",
       "      <td>2024-02-16 08:00:27.641336+00:00</td>\n",
       "      <td>713573</td>\n",
       "      <td>816775</td>\n",
       "      <td>1</td>\n",
       "      <td>0.757978</td>\n",
       "      <td>10320</td>\n",
       "      <td>10322</td>\n",
       "      <td>10320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5649</td>\n",
       "      <td>WSL Season 8</td>\n",
       "      <td>asia</td>\n",
       "      <td>tier2</td>\n",
       "      <td>2024-02-16 08:35:37.039547+00:00</td>\n",
       "      <td>713573</td>\n",
       "      <td>816776</td>\n",
       "      <td>2</td>\n",
       "      <td>0.757978</td>\n",
       "      <td>10320</td>\n",
       "      <td>10322</td>\n",
       "      <td>10322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5649</td>\n",
       "      <td>WSL Season 8</td>\n",
       "      <td>asia</td>\n",
       "      <td>tier2</td>\n",
       "      <td>2024-02-16 09:52:39.177390+00:00</td>\n",
       "      <td>713573</td>\n",
       "      <td>816777</td>\n",
       "      <td>3</td>\n",
       "      <td>0.738443</td>\n",
       "      <td>10320</td>\n",
       "      <td>10322</td>\n",
       "      <td>10320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5649</td>\n",
       "      <td>WSL Season 8</td>\n",
       "      <td>asia</td>\n",
       "      <td>tier2</td>\n",
       "      <td>2024-02-16 09:45:53.364959+00:00</td>\n",
       "      <td>713574</td>\n",
       "      <td>816779</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703136</td>\n",
       "      <td>10138</td>\n",
       "      <td>10321</td>\n",
       "      <td>10138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5649</td>\n",
       "      <td>WSL Season 8</td>\n",
       "      <td>asia</td>\n",
       "      <td>tier2</td>\n",
       "      <td>2024-02-16 12:22:35.717617+00:00</td>\n",
       "      <td>713574</td>\n",
       "      <td>816780</td>\n",
       "      <td>2</td>\n",
       "      <td>0.703136</td>\n",
       "      <td>10138</td>\n",
       "      <td>10321</td>\n",
       "      <td>10321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  tournament_id tournament_name region   tier  \\\n",
       "0           0           5649    WSL Season 8   asia  tier2   \n",
       "1           1           5649    WSL Season 8   asia  tier2   \n",
       "2           2           5649    WSL Season 8   asia  tier2   \n",
       "3           3           5649    WSL Season 8   asia  tier2   \n",
       "4           4           5649    WSL Season 8   asia  tier2   \n",
       "\n",
       "                         date_start  match_id  map_id  map_order  \\\n",
       "0  2024-02-16 08:00:27.641336+00:00    713573  816775          1   \n",
       "1  2024-02-16 08:35:37.039547+00:00    713573  816776          2   \n",
       "2  2024-02-16 09:52:39.177390+00:00    713573  816777          3   \n",
       "3  2024-02-16 09:45:53.364959+00:00    713574  816779          1   \n",
       "4  2024-02-16 12:22:35.717617+00:00    713574  816780          2   \n",
       "\n",
       "   prematch_home  home_team_id  away_team_id  map_win_team_id  \\\n",
       "0       0.757978         10320         10322            10320   \n",
       "1       0.757978         10320         10322            10322   \n",
       "2       0.738443         10320         10322            10320   \n",
       "3       0.703136         10138         10321            10138   \n",
       "4       0.703136         10138         10321            10321   \n",
       "\n",
       "   results_away_kills  results_home_kills  results_duration  \n",
       "0                 NaN                 NaN               NaN  \n",
       "1                 NaN                 NaN               NaN  \n",
       "2                 NaN                 NaN               NaN  \n",
       "3                 NaN                 NaN               NaN  \n",
       "4                 NaN                 NaN               NaN  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"mlbb_data_oddin.csv\")\n",
    "data.head() #first looks, can see null cols etc.. dataset is pretty small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639e30e-c02f-428c-9265-4ff4d2bc2240",
   "metadata": {},
   "source": [
    "I have worked with MOBA data for a while, so I'm pretty familiar with a lot of the game data and their nature, I will explain some things, but I might skip over others. Ask me about anything you see me do, and I have no problem talking about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2b99fcc-ee26-4fad-a663-127c6a3ae165",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Unnamed: 0',axis=1,inplace=True) # pre cleaning\n",
    "\n",
    "data['date_start'] = pd.to_datetime(data['date_start'])\n",
    "data = data.sort_values('date_start')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e49cc-929d-405a-922e-76dbd4ec8297",
   "metadata": {},
   "source": [
    "First thing that jumps out is that there is no game stat data, however there is a prematch probability, which I can use as a stand in for relative power rating, let's see how accurate it is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5e7106c1-bf47-4110-9428-92d8a5dfa3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6509025270758123"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_rate = data.dropna(subset=['prematch_home','map_win_team_id']) #drop\n",
    "c1 = win_rate['prematch_home'] == .50\n",
    "win_rate = win_rate[~c1]\n",
    "\n",
    "x1 = win_rate['home_team_id'] == win_rate['map_win_team_id']\n",
    "x2 = win_rate['prematch_home'] > .50\n",
    "\n",
    "y1 = win_rate['home_team_id'] != win_rate['map_win_team_id']\n",
    "y2 = win_rate['prematch_home'] < .50\n",
    "\n",
    "len(win_rate[(x1&x2|(y1&y2))])/len(win_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50c8fe-d063-4216-9c0e-d3dfdc755b2e",
   "metadata": {},
   "source": [
    "65 is very good for map predictions. Are these opening lines? closing? Something else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ef51533-58c8-4759-b8b6-88024aa32c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tournament_id           0\n",
       "tournament_name         0\n",
       "region                  0\n",
       "tier                    0\n",
       "date_start              1\n",
       "match_id                0\n",
       "map_id                  0\n",
       "map_order               0\n",
       "prematch_home           1\n",
       "home_team_id            0\n",
       "away_team_id            0\n",
       "map_win_team_id         0\n",
       "results_away_kills    469\n",
       "results_home_kills    469\n",
       "results_duration      469\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70570ef6-ab26-4450-a7a9-c437c4c0a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['date_start','prematch_home','results_away_kills','results_home_kills','results_duration'],inplace=True)\n",
    "\n",
    "nill_game = (data['results_away_kills'] == 0) & (data['results_home_kills'] == 0) # maps with 0-0 score probably not valid\n",
    "good_time = data['results_duration'] > 5 # maps under 5 seconds probably not valid\n",
    "\n",
    "data = data[~nill_game&good_time]\n",
    "data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ee71a-60e0-4940-aafe-cda00b0f84fb",
   "metadata": {},
   "source": [
    "Only having about 2000 good rows to work with is tough, it makes a lot of feature engineering I like to do moot. \n",
    "\n",
    "I can't really generate team specific data, since filtering for teams that have a 3 map history cuts the data almost by half. Also overfitting becomes more a problem, although I argue that overfitting these models is hard, if you use common sense.\n",
    "\n",
    "I am already thinking of how to get more data, but let's continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55362295-6ffc-423d-9228-e97f72206e19",
   "metadata": {},
   "source": [
    "# Feature Engineering and Modeling Cycle\n",
    "\n",
    "This is where the process is very iterative, I'll constantly be going back and changing things, deleting etc.. My real process is usually generating sometimes hundreds of features, then slowing peeling away useless and redundant features, until I find the core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "093631ed-c0b3-41bc-9175-c646a14cdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable\n",
    "data['kill_differential'] = data['results_home_kills'] - data['results_away_kills'] #target\n",
    "\n",
    "#will be main feature for baseline model\n",
    "data['prematch_away'] = 1 - data['prematch_home']\n",
    "data['rating_differential'] = data['prematch_home'] - data['prematch_away']\n",
    "\n",
    "#will be used as feature later on\n",
    "data['abs_kill_diff'] = abs(data['results_home_kills'] - data['results_away_kills'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81998da-1d3e-4a3d-a62e-40094a75e79b",
   "metadata": {},
   "source": [
    "## EDA example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4bd293cb-677b-4098-85e4-24bc4ac5d825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2071.000000\n",
       "mean        8.193626\n",
       "std         4.536026\n",
       "min         0.000000\n",
       "25%         5.000000\n",
       "50%         8.000000\n",
       "75%        11.000000\n",
       "max        28.000000\n",
       "Name: kill_differential, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(data['kill_differential']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63c14f3e-e1a1-48b8-92ea-90ca60b51ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tier\n",
       "tier1    0.597403\n",
       "tier2    0.724044\n",
       "tier3    0.929348\n",
       "Name: kill_differential, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do lower level teams have bigger kill differentials, it looks almost like it....\n",
    "data.groupby('tier')['kill_differential'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d809cbf-5f76-4937-9d47-dd5ccfcc757f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tier\n",
       "tier1    1155\n",
       "tier2     732\n",
       "tier3     184\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not enough data here I feel to make a feature, but we can try.\n",
    "data['tier'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61e54591-da85-4218-a465-e225877b7cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tier\n",
       "tier1    0.047066\n",
       "tier2    0.107215\n",
       "tier3    0.027606\n",
       "Name: rating_differential, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('tier')['rating_differential'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7df148-9df5-4170-851f-3ffc9d763bf2",
   "metadata": {},
   "source": [
    "Now I  ask questions like, do lower ranked teams have more blowouts? Stuff like this. Looking at tiers is interesting, but I do not feel like we have enough data, so its not something I pursue. Also in my previous models tier never really made a difference for me. However I was working on match prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a24cf-177e-4b45-b833-fa329b826268",
   "metadata": {},
   "source": [
    "This is where I start to think about team stats, moving averages etc.. however I do not feel that we have enough data to generate those stats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd2049-838c-4892-96ca-5f0f4c1088b9",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6de88032-5878-4b44-bbde-543c9abedde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5-Fold Cross-Validation Scores (Accuracy) ---\n",
      "[-8.67901234 -8.79667425 -8.72654683 -8.4512976  -9.25246344]\n",
      "\n",
      "Mean CV Accuracy: -8.7812\n"
     ]
    }
   ],
   "source": [
    "#Let's get a baseline with a LR simply using rating differential\n",
    "#test = data.dropna(subset=['hist_abs_90D_mean', 'home_hist_dur_6M', 'away_hist_dur_6M', 'hist_6M_std']).copy()\n",
    "#the test is code that I would use as I make new features that would have nulls, so I would have to remove them from the baseline model as well\n",
    "X = data[['rating_differential']]\n",
    "y = data['kill_differential']\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5,              \n",
    "    scoring='neg_root_mean_squared_error'  \n",
    ")\n",
    "\n",
    "print(f\"--- 5-Fold Cross-Validation Scores (Accuracy) ---\")\n",
    "print(cv_scores)\n",
    "print(f\"\\nMean CV Accuracy: {np.mean(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29097d2-a55f-4907-8ebb-9812ace387ce",
   "metadata": {},
   "source": [
    "This is where it hits me that I really do not have a solid metric to go by. In my other experience I would back test the model vs the betting odds and get my baseline, but I realize here I do not have a special metric like ROI.\n",
    "\n",
    "\n",
    "I always evaluated my model somewhat intuitively, in the sense, that if a model had lower log log loss and a higher accuracy, and if the ROI was lower, you had to find a compromise. \n",
    "\n",
    "\n",
    "Now here I am dealing with an even more ambiguous metrics MAE or RMSE, how good is good? I have a feeling you must have some \"cash\" metric that evaluates the \"value\" of a line somehow. Interesting to think about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eba11af2-4bef-43cf-9e5b-bba7ae5fe251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ususally wouldn't do this, since I like to be flexible, but I will use this basic model to show you my process, and I'd like call it a few times.\n",
    "#simple model with CV etc.\n",
    "def evaluate_model(model, features, data, target='kill_differential'):\n",
    "    \"\"\"\n",
    "    Runs 5-fold CV for a given model and feature set,\n",
    "    printing the mean RMSE. \n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Prepare data - drop NaNs only from relevant columns\n",
    "    eval_data = data.dropna(subset=features + [target])\n",
    "    X = eval_data[features]\n",
    "    y = eval_data[target]\n",
    "    \n",
    "    # 2. Define CV strategy (consistent across all models)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=99)\n",
    "    \n",
    "    # 3. Run CV and get scores\n",
    "    scores = cross_val_score(\n",
    "        estimator=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=cv,              \n",
    "        scoring='neg_root_mean_squared_error'  \n",
    "    )\n",
    "    # 4. Print results\n",
    "    print(f\"--- Testing Features: {features} ---\")\n",
    "    print(f\"CV Scores (RMSE): {scores}\")\n",
    "    print(f\"Mean CV RMSE: {np.mean(scores):.4f}\\n\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840997f5-a6ba-4b4c-9956-22804e71aacb",
   "metadata": {},
   "source": [
    "## Creation of some Team and Population stats\n",
    "\n",
    "These are the stats that I have to work with if we have a small sparse data set. This cell below gives me the duration and kill differential population data. Duration obviously longer map == more kills. I already have an understanding of what creates high kill maps and I am exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6dee0b51-15a5-43ec-9384-0aca66f8fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_stats_dict = {} #this will hold the data we produce\n",
    "\n",
    "# I iterate through the dataset since I have to only pull data prematch to prevent data leakage, using vectors here will not work\n",
    "for index, row in data.iterrows():\n",
    "\n",
    "    #these variables help us create the historical window below with no data leakage, 6 months\n",
    "    current_date = row['date_start']\n",
    "    current_match_id = row['match_id']\n",
    "    cutoff_date = current_date - pd.DateOffset(days=180)\n",
    "    \n",
    "    #hisorical window where we generate the stats\n",
    "    historical_data_90D = data[\n",
    "        (data['date_start'] < current_date) &     # Data is in the past\n",
    "        (data['date_start'] >= cutoff_date) &    # Data is within the 90-day window\n",
    "        (data['match_id'] != current_match_id)  #EXCLUDE maps from the current match\n",
    "    ]\n",
    "\n",
    "    \n",
    "    stats_kill_diff = historical_data_90D['abs_kill_diff'].describe() # getting the describe data\n",
    "    stats_duration = historical_data_90D['results_duration'].describe()\n",
    "        \n",
    "    historical_stats_dict[row['map_id']] = {\n",
    "        'hist_abs_180D_mean': stats_kill_diff['mean'],\n",
    "        'hist_abs_180D_std': stats_kill_diff['std'],\n",
    "        'hist_abs_180D_min': stats_kill_diff['min'],\n",
    "        'hist_abs_180D_max': stats_kill_diff['max'],\n",
    "        'hist_abs_180D_q25': stats_kill_diff['25%'],\n",
    "        'hist_abs_180D_q50': stats_kill_diff['50%'],\n",
    "        'hist_abs_180D_q75': stats_kill_diff['75%'],\n",
    "        \n",
    "        'hist_dur_180D_mean': stats_duration['mean'],\n",
    "        'hist_dur_180D_std': stats_duration['std'],\n",
    "        'hist_dur_180D_min': stats_duration['min'],\n",
    "        'hist_dur_180D_max': stats_duration['max'],\n",
    "        'hist_dur_180D_q25': stats_duration['25%'],\n",
    "        'hist_dur_180D_q50': stats_duration['50%'],\n",
    "        'hist_dur_180D_q75': stats_duration['75%']\n",
    "    }\n",
    "\n",
    "#Convert results and merge back\n",
    "features_df = pd.DataFrame.from_dict(historical_stats_dict, orient='index')\n",
    "features_df = features_df.reset_index().rename(columns={'index': 'map_id'})\n",
    "data = data.merge(features_df, on='map_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6221b92-fbb1-45e6-955e-988af1a10318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe makes it easy to create team history data, I have done this many ways, but trying this out.\n",
    "\n",
    "# This creates a new df with just the home team's perspective\n",
    "df_home = data.assign(\n",
    "    team_id=data['home_team_id'],\n",
    "    map_duration=data['results_duration'],\n",
    "    team_kill_diff=data['kill_differential'] # Home's perspective\n",
    ")[['map_id', 'date_start', 'team_id', 'map_duration', 'team_kill_diff']]\n",
    "\n",
    "\n",
    "# This creates a new df with the away team's perspective\n",
    "df_away = data.assign(\n",
    "    team_id=data['away_team_id'],\n",
    "    map_duration=data['results_duration'],\n",
    "    team_kill_diff=-data['kill_differential'] # Flipped for Away's perspective\n",
    ")[['map_id', 'date_start', 'team_id', 'map_duration', 'team_kill_diff']]\n",
    "\n",
    "\n",
    "# This stacks them, so a 1000-row 'data' becomes a 2000-row 'team_game_data'\n",
    "team_game_data = pd.concat([df_home, df_away])\n",
    "\n",
    "# Sort by date\n",
    "team_game_data = team_game_data.sort_values('date_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1cacedf4-c37c-4c23-b8e9-a86bd805ee81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_id</th>\n",
       "      <th>date_start</th>\n",
       "      <th>team_id</th>\n",
       "      <th>map_duration</th>\n",
       "      <th>team_kill_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946460</td>\n",
       "      <td>2024-04-11 07:02:56.516165+00:00</td>\n",
       "      <td>10697</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946460</td>\n",
       "      <td>2024-04-11 07:02:56.516165+00:00</td>\n",
       "      <td>10691</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>946461</td>\n",
       "      <td>2024-04-11 07:39:30.213183+00:00</td>\n",
       "      <td>10691</td>\n",
       "      <td>879.0</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>946461</td>\n",
       "      <td>2024-04-11 07:39:30.213183+00:00</td>\n",
       "      <td>10697</td>\n",
       "      <td>879.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>946462</td>\n",
       "      <td>2024-04-11 08:14:51.920556+00:00</td>\n",
       "      <td>10697</td>\n",
       "      <td>673.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   map_id                       date_start  team_id  map_duration  \\\n",
       "0  946460 2024-04-11 07:02:56.516165+00:00    10697        1234.0   \n",
       "0  946460 2024-04-11 07:02:56.516165+00:00    10691        1234.0   \n",
       "1  946461 2024-04-11 07:39:30.213183+00:00    10691         879.0   \n",
       "1  946461 2024-04-11 07:39:30.213183+00:00    10697         879.0   \n",
       "2  946462 2024-04-11 08:14:51.920556+00:00    10697         673.0   \n",
       "\n",
       "   team_kill_diff  \n",
       "0            -8.0  \n",
       "0             8.0  \n",
       "1            -8.0  \n",
       "1             8.0  \n",
       "2             9.0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_game_data.head() #we use this in the code below to generate team stats..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e67eb363-1943-4c74-84ae-8be97f2cfff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to store new features\n",
    "historical_stats_dict = {}\n",
    "\n",
    "#same deal as before, but now we input rolling averages of a teams kill diff and duration\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # Get current match info\n",
    "    current_date = row['date_start']\n",
    "    current_match_id = row['match_id']\n",
    "    home_team_id = row['home_team_id']\n",
    "    away_team_id = row['away_team_id']\n",
    "    \n",
    "\n",
    "    cutoff_date = current_date - pd.DateOffset(days=180)\n",
    "\n",
    "    #historical stats for home team\n",
    "    home_hist_slice = team_game_data[\n",
    "        (team_game_data['team_id'] == home_team_id) & # Filter for this team\n",
    "        (team_game_data['date_start'] < current_date) &  # Data is in the past\n",
    "        (team_game_data['date_start'] >= cutoff_date) & # Data is in 6-month window\n",
    "        (team_game_data['map_id'] != row['map_id'])      # <-- No data leakage\n",
    "\n",
    "\n",
    "    ]\n",
    "    \n",
    "    #historical stats for away team\n",
    "    away_hist_slice = team_game_data[\n",
    "        (team_game_data['team_id'] == away_team_id) & \n",
    "        (team_game_data['date_start'] < current_date) & \n",
    "        (team_game_data['date_start'] >= cutoff_date) & \n",
    "        (team_game_data['map_id'] != row['map_id'])  \n",
    "    ]\n",
    "    \n",
    "    historical_stats_dict[row['map_id']] = { \n",
    "        'home_hist_kd_6M': home_hist_slice['team_kill_diff'].mean(),\n",
    "        'home_hist_dur_6M': home_hist_slice['map_duration'].mean(),\n",
    "        'away_hist_kd_6M': away_hist_slice['team_kill_diff'].mean(),\n",
    "        'away_hist_dur_6M': away_hist_slice['map_duration'].mean()\n",
    "    }\n",
    "\n",
    "features_df = pd.DataFrame.from_dict(historical_stats_dict, orient='index')\n",
    "features_df = features_df.reset_index().rename(columns={'index': 'map_id'})\n",
    "data = data.merge(features_df, on='map_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "67e38689-c8d6-40b6-a9c5-1c2c7ba150f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tournament_id', 'tournament_name', 'region', 'tier', 'date_start',\n",
       "       'match_id', 'map_id', 'map_order', 'prematch_home', 'home_team_id',\n",
       "       'away_team_id', 'map_win_team_id', 'results_away_kills',\n",
       "       'results_home_kills', 'results_duration', 'kill_differential',\n",
       "       'prematch_away', 'rating_differential', 'abs_kill_diff',\n",
       "       'hist_abs_180D_mean', 'hist_abs_180D_std', 'hist_abs_180D_min',\n",
       "       'hist_abs_180D_max', 'hist_abs_180D_q25', 'hist_abs_180D_q50',\n",
       "       'hist_abs_180D_q75', 'hist_dur_180D_mean', 'hist_dur_180D_std',\n",
       "       'hist_dur_180D_min', 'hist_dur_180D_max', 'hist_dur_180D_q25',\n",
       "       'hist_dur_180D_q50', 'hist_dur_180D_q75', 'home_hist_kd_6M',\n",
       "       'home_hist_dur_6M', 'away_hist_kd_6M', 'away_hist_dur_6M'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc617bc-0789-4e74-9b71-376dde147504",
   "metadata": {},
   "source": [
    "When I am creating these rolling windows, I usually make many more. Now I prefer to use decay curves and weighted averages. I've found decay functions to be a little better than bagging a bunch of static windows, but I wanted to keep this a little simpler. \n",
    "\n",
    "With only a week, it wasn't practical to go with my usual testing of decay curves and time windows. You get the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3406d8-ba6f-45a6-b60d-1353bf7e3580",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6652ebe4-2c68-4e9c-a8d5-d5e7cf8cf4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Features: ['rating_differential', 'sum_dur'] ---\n",
      "CV Scores (RMSE): [-8.89905808 -9.09714769 -8.74134163 -8.773837   -8.21536366]\n",
      "Mean CV RMSE: -8.7453\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "data['sum_dur'] = data['home_hist_dur_6M'] + data['away_hist_dur_6M'] #new feat, sum of both teams rolling mean duration\n",
    "\n",
    "#calling model again with new feats\n",
    "features = ['rating_differential','sum_dur']\n",
    "model = LinearRegression()\n",
    "evaluate_model(model = model , features=features, data=data, target='kill_differential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930cc31-5821-442c-9f85-c8c150b93292",
   "metadata": {},
   "source": [
    "#### -8.7812 ---> -8.7543"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e49d86-e947-41bf-a452-f6eb38322f21",
   "metadata": {},
   "source": [
    "Sorry if things get messy and repetative with the code, I am trying to show you visually how I would test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2c074df5-464b-4a44-9e6c-51924653a32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tournament_id', 'tournament_name', 'region', 'tier', 'date_start',\n",
       "       'match_id', 'map_id', 'map_order', 'prematch_home', 'home_team_id',\n",
       "       'away_team_id', 'map_win_team_id', 'results_away_kills',\n",
       "       'results_home_kills', 'results_duration', 'kill_differential',\n",
       "       'prematch_away', 'rating_differential', 'abs_kill_diff',\n",
       "       'hist_abs_180D_mean', 'hist_abs_180D_std', 'hist_abs_180D_min',\n",
       "       'hist_abs_180D_max', 'hist_abs_180D_q25', 'hist_abs_180D_q50',\n",
       "       'hist_abs_180D_q75', 'hist_dur_180D_mean', 'hist_dur_180D_std',\n",
       "       'hist_dur_180D_min', 'hist_dur_180D_max', 'hist_dur_180D_q25',\n",
       "       'hist_dur_180D_q50', 'hist_dur_180D_q75', 'home_hist_kd_6M',\n",
       "       'home_hist_dur_6M', 'away_hist_kd_6M', 'away_hist_dur_6M', 'sum_dur'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns #what we have going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8c3642c5-051f-4625-ae0f-3271b72920e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Features: ['rating_differential', 'sum_dur', 'hist_abs_180D_std'] ---\n",
      "CV Scores (RMSE): [-8.96406381 -8.51710277 -8.56680673 -9.00187629 -8.7680716 ]\n",
      "Mean CV RMSE: -8.7636\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#calling model again with new feats\n",
    "features = ['rating_differential','sum_dur','hist_abs_180D_std']\n",
    "model = model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearRegression()\n",
    ") # adding scaling for the different features we are now adding....\n",
    "\n",
    "\n",
    "evaluate_model(model = model , features=features, data=data, target='kill_differential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0450b93-924c-4894-bb12-c4fc2b294911",
   "metadata": {},
   "source": [
    "Now I try adding other feats one by one, just playing around. The end goal is to have diverse feature sets, feeding each one to its \"best fit\" model, then taking OOF predictions to LR. No luck here so we move on. I will come back to these versions again and again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6702d3-daf1-4c16-ba8a-b555a213bfea",
   "metadata": {},
   "source": [
    "## Creation of RF OOF Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aebb967e-3369-4156-bb75-a12bf62b3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find with these feats a RF is a good starting point, we can of course tune this set and make these preds more powerful\n",
    "rf_feats = list(test.columns)[20:-5]\n",
    "X = data[rf_feats]\n",
    "y = data['kill_differential']\n",
    "\n",
    "\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators=100,  \n",
    "    random_state=99,  \n",
    ")\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=99)\n",
    "\n",
    "\n",
    "oof_predictions = cross_val_predict(\n",
    "    estimator=model_rf,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    ")\n",
    "\n",
    "data['rf_preds'] = oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "282ab52c-dffd-4d3d-b9ea-eedd7c86a512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tournament_id', 'tournament_name', 'region', 'tier', 'date_start',\n",
       "       'match_id', 'map_id', 'map_order', 'prematch_home', 'home_team_id',\n",
       "       'away_team_id', 'map_win_team_id', 'results_away_kills',\n",
       "       'results_home_kills', 'results_duration', 'kill_differential',\n",
       "       'prematch_away', 'rating_differential', 'abs_kill_diff',\n",
       "       'hist_abs_180D_mean', 'hist_abs_180D_std', 'hist_abs_180D_min',\n",
       "       'hist_abs_180D_max', 'hist_abs_180D_q25', 'hist_abs_180D_q50',\n",
       "       'hist_abs_180D_q75', 'hist_dur_180D_mean', 'hist_dur_180D_std',\n",
       "       'hist_dur_180D_min', 'hist_dur_180D_max', 'hist_dur_180D_q25',\n",
       "       'hist_dur_180D_q50', 'hist_dur_180D_q75', 'home_hist_kd_6M',\n",
       "       'home_hist_dur_6M', 'away_hist_kd_6M', 'away_hist_dur_6M', 'sum_dur',\n",
       "       'rf_preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef8a78b4-4cf1-465b-b2e1-6cad86ce548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Features: ['rating_differential', 'sum_dur', 'rf_preds'] ---\n",
      "CV Scores (RMSE): [-8.84924525 -9.11120555 -8.71216334 -8.7346379  -8.23354314]\n",
      "Mean CV RMSE: -8.7282\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#calling model again with new OOF preds\n",
    "features = ['rating_differential','sum_dur','rf_preds']\n",
    "\n",
    "model = model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearRegression()\n",
    ") # adding scaling for the different features we are now adding....\n",
    "\n",
    "\n",
    "evaluate_model(model = model , features=features, data=data, target='kill_differential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9cd3c7-ae46-42c1-b02b-6d981a3fcc39",
   "metadata": {},
   "source": [
    "#### -8.7453 ---> -8.7282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "08dcb064-0e21-46b9-a7ca-a5047d5273b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Features: ['rating_differential', 'rf_preds', 'sum_dur'] ---\n",
      "CV Scores (RMSE): [-8.85488123 -9.11341929 -8.71138789 -8.72693628 -8.2366708 ]\n",
      "Mean CV RMSE: -8.7287\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "base_model_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "bagging_model = BaggingRegressor(\n",
    "    estimator=base_model_pipeline,\n",
    "    n_estimators=20,   \n",
    "    random_state=99,\n",
    "    max_samples = .90,\n",
    "    max_features = 1.0,\n",
    ")\n",
    "\n",
    "# --- 3. Define Feature List ---\n",
    "features = ['rating_differential', 'rf_preds', 'sum_dur',]\n",
    "\n",
    "\n",
    "evaluate_model(model=bagging_model, features=features, data=data,target='kill_differential')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640b6f85-c233-47e6-a025-fbe522eb2464",
   "metadata": {},
   "source": [
    "No luck with bagging, but it usually works with more features.\n",
    "\n",
    "\n",
    "\n",
    "This is where I would then return to feature engineering. Working my way through this pipeline again and again, adding new model OOF preds creating new features, combining them, removing them, placing them into different models, usually all to end with a bagged LR. When the features get very numerous I'd use algos do build feature lists, there are some very cool auto-ML, auto-ensemble packages, I've been wanting to test. All the time also looking for signs of overfitting, data leakage etc..\n",
    "\n",
    "\n",
    "\n",
    "This is something that is hard to show in a week of work, but I think this is a good idea of things. This process would remain the same but escalating over the next weeks in complexity. With a few more weeks, I'd spend time watching the sport, researching. I have no doubt, that further work would lead to better scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afcc4fb-569d-4431-afa0-bdc7b7b9dac7",
   "metadata": {},
   "source": [
    "## Creation of New Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2da97-5571-441f-bb83-dd37b9836c6f",
   "metadata": {},
   "source": [
    "I felt really limited by my lack of instances and features, and really could not find a serious database, or even the ability to scrape one. \n",
    "\n",
    "\n",
    "\n",
    "So I had a an idea to pull match data from Youtube streams. In retrospect, this was a bit large of a project to complete in the time allowed, but I am happy with it, however.... It worked until Youtube flagged my cloud account and I lost my data. \n",
    "\n",
    "\n",
    "\n",
    "Here was how it worked. I spent a lot of energy on this, I thought it was worth it, even though I did not get to use it. I think it is a solid idea to pursue.\n",
    "\n",
    "\n",
    "\n",
    "This is where my edge has come from over the years. Doing the things that others would consider not worth it or a pain in the butt. Which this was, but data is really the easiest and biggest way to make an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a697428-efc2-4568-bf1a-98493e351457",
   "metadata": {},
   "source": [
    "## Youtube to Detailed Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d156d9-6139-472a-83e4-b1f2e8ebe324",
   "metadata": {},
   "source": [
    "I will not get into too much detail, but this is basically how it worked. \n",
    "\n",
    "\n",
    "\n",
    "### 2024 fixtures from Liquipedia - All S, A, and B tier tournament fixtures.\n",
    "\n",
    "* contains basic fixture data, plus hero selections. Around 6000 solid fixtures now.\n",
    "\n",
    "* This serves as the skeleton to fill\n",
    "\n",
    "\n",
    "\n",
    "### Find the streams for every tournament\n",
    "\n",
    "* easier than it sounds. Find over a thousand youtube urls from the 2024 tournaments\n",
    "\n",
    "\n",
    "\n",
    "### Download youtube videos and extract \"still\" Frames\n",
    "\n",
    "* frames are graded on pixel motion, so its easy to extract what we need. \n",
    "\n",
    "\n",
    "\n",
    "### Frames are sent to a CNN classifier\n",
    "\n",
    "* removes 90% of junk screens to save money\n",
    "\n",
    "\n",
    "\n",
    "### Final map stat screens are sent to a Gemini model\n",
    "\n",
    "* stat screens are parsed, json is saved with the team gold etc...\n",
    "\n",
    "\n",
    "\n",
    "### Youtube data is matched to Liquipedia to produce data set.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558e001-21cc-4e6d-8e29-80bb9a0c5a27",
   "metadata": {},
   "source": [
    "Since that did not work out, I decided to work with the Liquipedia dataset and have some fun results modeling duration. See the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fedcb3-8c3c-440f-abec-fb4676138846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Pip)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
