{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787b5bf-361a-4bc4-b239-a449afa5d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelHelpers - shared utilities for model training and evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold, cross_val_score, GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def evaluate_model(model, features, data, target='duration'):\n",
    "    \"\"\"Runs 5-fold CV for a given model and feature set, printing the mean RMSE.\"\"\"\n",
    "    eval_data = data.dropna(subset=features + [target])\n",
    "    X = eval_data[features]\n",
    "    y = eval_data[target]\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=99)\n",
    "    \n",
    "    scores = cross_val_score(\n",
    "        estimator=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=cv,              \n",
    "        scoring='neg_root_mean_squared_error'  \n",
    "    )\n",
    "    \n",
    "    print(f\"CV Scores (RMSE): {scores}\")\n",
    "    print(f\"Mean CV RMSE: {np.mean(scores):.4f}\\n\")\n",
    "\n",
    "\n",
    "# Constants for padding hero lists\n",
    "HERO_LIST_MAX_LEN = 5 \n",
    "PADDING_VALUE = 0 \n",
    "TARGET_COLUMN = 'duration'\n",
    "\n",
    "\n",
    "class MatchDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for match data with hero picks/bans padded to fixed length.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, target_column):\n",
    "        self.df = dataframe\n",
    "        self.team1_ids = self.df['team1_id'].values\n",
    "        self.team2_ids = self.df['team2_id'].values\n",
    "        self.team1_side_ids = self.df['team1_side_id'].values\n",
    "        self.team2_side_ids = self.df['team2_side_id'].values\n",
    "        self.t1_picks = self.df['team1_picks_ids'].values\n",
    "        self.t2_picks = self.df['team2_picks_ids'].values\n",
    "        self.t1_bans = self.df['team1_bans_ids'].values\n",
    "        self.t2_bans = self.df['team2_bans_ids'].values\n",
    "        self.target = self.df[target_column].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pad_hero_list(self, hero_list):\n",
    "        \"\"\"Pad or truncate hero list to HERO_LIST_MAX_LEN.\"\"\"\n",
    "        padded_list = hero_list + [PADDING_VALUE] * (HERO_LIST_MAX_LEN - len(hero_list))\n",
    "        return padded_list[:HERO_LIST_MAX_LEN]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        team1_id = torch.tensor(self.team1_ids[idx], dtype=torch.long)\n",
    "        team2_id = torch.tensor(self.team2_ids[idx], dtype=torch.long)\n",
    "        team1_side_id = torch.tensor(self.team1_side_ids[idx], dtype=torch.long)\n",
    "        team2_side_id = torch.tensor(self.team2_side_ids[idx], dtype=torch.long)\n",
    "\n",
    "        t1_picks = torch.tensor(self._pad_hero_list(self.t1_picks[idx]), dtype=torch.long)\n",
    "        t2_picks = torch.tensor(self._pad_hero_list(self.t2_picks[idx]), dtype=torch.long)\n",
    "        t1_bans = torch.tensor(self._pad_hero_list(self.t1_bans[idx]), dtype=torch.long)\n",
    "        t2_bans = torch.tensor(self._pad_hero_list(self.t2_bans[idx]), dtype=torch.long)\n",
    "        \n",
    "        target = torch.tensor(self.target[idx], dtype=torch.float)\n",
    "\n",
    "        features = {\n",
    "            'team1_id': team1_id,\n",
    "            'team2_id': team2_id,\n",
    "            'team1_side_id': team1_side_id,\n",
    "            'team2_side_id': team2_side_id,\n",
    "            't1_picks': t1_picks,\n",
    "            't2_picks': t2_picks,\n",
    "            't1_bans': t1_bans,\n",
    "            't2_bans': t2_bans\n",
    "        }\n",
    "        \n",
    "        return features, target\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple model where pick/ban order does not matter.\n",
    "    It uses EmbeddingBag to sum hero embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_teams, n_heroes, n_sides, \n",
    "                 team_embed_dim=16, hero_embed_dim=16, side_embed_dim=2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # --- 1. Define Embedding Layers ---\n",
    "        \n",
    "        # Simple embeddings for single items\n",
    "        # padding_idx=0 means the vector for ID 0 will always be zeros\n",
    "        # and won't be updated during training.\n",
    "        self.team_embed = nn.Embedding(n_teams, team_embed_dim, padding_idx=0)\n",
    "        self.side_embed = nn.Embedding(n_sides, side_embed_dim, padding_idx=0)\n",
    "        \n",
    "        # EmbeddingBag for lists (picks/bans)\n",
    "        # This is the key part for \"order doesn't matter\".\n",
    "        # It sums the embeddings of all heroes in the list.\n",
    "        self.hero_embed_bag = nn.EmbeddingBag(\n",
    "            n_heroes, \n",
    "            hero_embed_dim, \n",
    "            padding_idx=0, \n",
    "            mode='sum' # This is what ignores order\n",
    "        )\n",
    "        \n",
    "        # --- 2. Define Classifier Head ---\n",
    "        \n",
    "        # Calculate the total size of our concatenated vector\n",
    "        # For each team we have: 1 team_vec + 1 side_vec + 1 picks_vec + 1 bans_vec\n",
    "        # Total size = 2 * (team_dim + side_dim + picks_dim + bans_dim)\n",
    "        \n",
    "        input_size = 2 * (team_embed_dim + side_embed_dim + \n",
    "                          hero_embed_dim + hero_embed_dim)\n",
    "        \n",
    "        # A simple stack of linear layers\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1) # Output a single number (logit)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3) # Helps prevent overfitting\n",
    "\n",
    "    def forward(self, features):\n",
    "        \n",
    "        # --- 1. Get Embeddings for Team 1 ---\n",
    "        # features['team1_id'] shape is [batch_size]\n",
    "        # t1_team_vec shape becomes [batch_size, team_embed_dim]\n",
    "        t1_team_vec = self.team_embed(features['team1_id'])\n",
    "        t1_side_vec = self.side_embed(features['team1_side_id'])\n",
    "        \n",
    "        # features['t1_picks'] shape is [batch_size, 5]\n",
    "        # t1_picks_vec shape becomes [batch_size, hero_embed_dim]\n",
    "        t1_picks_vec = self.hero_embed_bag(features['t1_picks'])\n",
    "        t1_bans_vec = self.hero_embed_bag(features['t1_bans'])\n",
    "        \n",
    "        # --- 2. Get Embeddings for Team 2 ---\n",
    "        t2_team_vec = self.team_embed(features['team2_id'])\n",
    "        t2_side_vec = self.side_embed(features['team2_side_id'])\n",
    "        t2_picks_vec = self.hero_embed_bag(features['t2_picks'])\n",
    "        t2_bans_vec = self.hero_embed_bag(features['t2_bans'])\n",
    "        \n",
    "        # --- 3. Concatenate all vectors into one big vector ---\n",
    "        x = torch.cat([\n",
    "            t1_team_vec, t1_side_vec, t1_picks_vec, t1_bans_vec,\n",
    "            t2_team_vec, t2_side_vec, t2_picks_vec, t2_bans_vec\n",
    "        ], dim=1) # dim=1 to stack columns horizontally\n",
    "        # Final shape of x is [batch_size, input_size]\n",
    "        \n",
    "        # --- 4. Pass through the classifier ---\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output raw logit, shape [batch_size, 1]\n",
    "        x = self.fc3(x) \n",
    "        \n",
    "        # Squeeze from [batch_size, 1] to [batch_size]\n",
    "        # This makes it match our target's shape\n",
    "        return x.squeeze(dim=1)\n",
    "\n",
    "class PositionalModel(nn.Module):\n",
    "    # Removed n_games and game_embed_dim from parameters\n",
    "    def __init__(self, n_teams, n_heroes, n_sides,\n",
    "                 team_embed_dim=16, hero_embed_dim=16, \n",
    "                 side_embed_dim=2): \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # --- 1. Print received vocab sizes for debugging ---\n",
    "        print(f\"  [Model Init] n_teams: {n_teams}\")\n",
    "        print(f\"  [Model Init] n_heroes: {n_heroes}\")\n",
    "        print(f\"  [Model Init] n_sides: {n_sides}\")\n",
    "        # Removed n_games print\n",
    "        \n",
    "        # --- 2. Define Embedding Layers ---\n",
    "        self.team_embed = nn.Embedding(n_teams, team_embed_dim, padding_idx=0)\n",
    "        self.side_embed = nn.Embedding(n_sides, side_embed_dim, padding_idx=0)\n",
    "        self.hero_embed = nn.Embedding(n_heroes, hero_embed_dim, padding_idx=0)\n",
    "        \n",
    "        n_positions = HERO_LIST_MAX_LEN + 1\n",
    "        print(f\"  [Model Init] n_positions: {n_positions}\")\n",
    "        self.position_embed = nn.Embedding(n_positions, hero_embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Removed self.game_embed\n",
    "\n",
    "        # --- 3. Define Classifier Head ---\n",
    "        # Removed game_embed_dim from input_size calculation\n",
    "        input_size = 2 * (team_embed_dim + side_embed_dim + \n",
    "                          hero_embed_dim + hero_embed_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1) \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def _get_draft_vector(self, hero_ids, pos_ids):\n",
    "        \"\"\"Helper to combine hero + position embeddings\"\"\"\n",
    "        hero_vecs = self.hero_embed(hero_ids)\n",
    "        pos_vecs = self.position_embed(pos_ids)\n",
    "        combined_vecs = hero_vecs + pos_vecs\n",
    "        final_draft_vec = combined_vecs.sum(dim=1)\n",
    "        return final_draft_vec\n",
    "\n",
    "    def forward(self, features):\n",
    "        \n",
    "        # --- 1. Get Single-Item Embeddings ---\n",
    "        t1_team_vec = self.team_embed(features['team1_id'])\n",
    "        t1_side_vec = self.side_embed(features['team1_side_id'])\n",
    "        t2_team_vec = self.team_embed(features['team2_id'])\n",
    "        t2_side_vec = self.side_embed(features['team2_side_id'])\n",
    "        # Removed game_vec\n",
    "\n",
    "        # --- 2. Get Draft/Ban Vectors ---\n",
    "        t1_picks_vec = self._get_draft_vector(features['t1_picks'], features['t1_picks_pos'])\n",
    "        t1_bans_vec = self._get_draft_vector(features['t1_bans'], features['t1_bans_pos'])\n",
    "        t2_picks_vec = self._get_draft_vector(features['t2_picks'], features['t2_picks_pos'])\n",
    "        t2_bans_vec = self._get_draft_vector(features['t2_bans'], features['t2_bans_pos'])\n",
    "        \n",
    "        # --- 3. Concatenate all vectors ---\n",
    "        # Removed game_vec from torch.cat\n",
    "        x = torch.cat([\n",
    "            t1_team_vec, t1_side_vec, t1_picks_vec, t1_bans_vec,\n",
    "            t2_team_vec, t2_side_vec, t2_picks_vec, t2_bans_vec\n",
    "        ], dim=1) \n",
    "        \n",
    "        # --- 4. Pass through the classifier ---\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x) \n",
    "        \n",
    "        return x.squeeze(dim=1)\n",
    "\n",
    "N_SPLITS = 5\n",
    "N_EPOCHS = 10 \n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_loop(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Runs one epoch of training.\"\"\"\n",
    "    model.train() \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for features, targets in loader:\n",
    "        features = {k: v.to(device) for k, v in features.items()}\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 2. Backward pass and optimization\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()       \n",
    "        optimizer.step()      \n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# (get_predictions is already correct from your paste)\n",
    "def get_predictions(model, loader, device):\n",
    "    \"\"\"Gets predictions for a validation/test set.\"\"\"\n",
    "    model.eval() \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for features, targets in loader:\n",
    "            features = {k: v.to(device) for k, v in features.items()}\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Get raw model output (which is our duration prediction)\n",
    "            outputs = model(features)\n",
    "            \n",
    "            # --- This is already correct! ---\n",
    "            preds = outputs \n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(all_preds), np.concatenate(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d5c06-822b-4e59-8111-7492be9111a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Pip)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
