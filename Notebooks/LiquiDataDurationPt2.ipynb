{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5b4805-d0b9-4318-b48c-9883c9a4c333",
   "metadata": {},
   "source": [
    "### Liquipedia Data Duration Modeling\n",
    "\n",
    "\n",
    "Considering I could not get the dataset I wanted, I moved to model map duration, which actually was very fun. I got to use some models I had never used before, but have wanted to. I tnhink this is a great market to model I wonder what the margins are here and how good the market is at predicting these.\n",
    "\n",
    "Liquipedia had lots of instances along with hero data, side etc.. Perfect for what I had in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e79850-fdfb-4d87-97fd-9497c7649278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from rankit.Table import Table\n",
    "from rankit.Ranker import EloRanker\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caae47af-1bfe-4190-a4f6-40120c07f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ModelHelpers.ipynb # this runs the help notebook to make things a bit cleaner here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2857e-b937-43f9-b944-9b152e427050",
   "metadata": {},
   "source": [
    "## Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19d261d-c5b5-4529-b61f-c78787085ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>map_id</th>\n",
       "      <th>game_number</th>\n",
       "      <th>tournament_name</th>\n",
       "      <th>tournament_url</th>\n",
       "      <th>tier</th>\n",
       "      <th>tournament_stage</th>\n",
       "      <th>liquipedia_path</th>\n",
       "      <th>match_date</th>\n",
       "      <th>team1</th>\n",
       "      <th>team2</th>\n",
       "      <th>winner</th>\n",
       "      <th>duration</th>\n",
       "      <th>team1_picks</th>\n",
       "      <th>team1_bans</th>\n",
       "      <th>team2_picks</th>\n",
       "      <th>team2_bans</th>\n",
       "      <th>team1_side</th>\n",
       "      <th>team2_side</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MATCH5ebf6583a5</td>\n",
       "      <td>MAP7466b3e7f36b</td>\n",
       "      <td>2</td>\n",
       "      <td>WSL Season 6</td>\n",
       "      <td>https://liquipedia.net/mobilelegends/WSL/Season_6</td>\n",
       "      <td>B-Tier</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>WSL/Season 6/Regular Season</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>Foes Win</td>\n",
       "      <td>GPX Basreng</td>\n",
       "      <td>2</td>\n",
       "      <td>12m 56s</td>\n",
       "      <td>hilda, bene, lylia, brody, lolita</td>\n",
       "      <td>wan, joy, gloo, estes, martis</td>\n",
       "      <td>lapu, akai, yve, karrie, atlas</td>\n",
       "      <td>aamon, fred, haya, ling, gs</td>\n",
       "      <td>red</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MATCH5ebf6583a5</td>\n",
       "      <td>MAP76eb4613f945</td>\n",
       "      <td>1</td>\n",
       "      <td>WSL Season 6</td>\n",
       "      <td>https://liquipedia.net/mobilelegends/WSL/Season_6</td>\n",
       "      <td>B-Tier</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>WSL/Season 6/Regular Season</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>Foes Win</td>\n",
       "      <td>GPX Basreng</td>\n",
       "      <td>2</td>\n",
       "      <td>13m 19s</td>\n",
       "      <td>masha, barats, cecilion, karrie, diggie</td>\n",
       "      <td>wan, haya, joy, valen, fara</td>\n",
       "      <td>fred, ling, xavier, brody, lolita</td>\n",
       "      <td>gloo, yve, aamon, akai, martis</td>\n",
       "      <td>blue</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MATCH85626e8803</td>\n",
       "      <td>MAPb668e4b7d592</td>\n",
       "      <td>2</td>\n",
       "      <td>WSL Season 6</td>\n",
       "      <td>https://liquipedia.net/mobilelegends/WSL/Season_6</td>\n",
       "      <td>B-Tier</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>WSL/Season 6/Regular Season</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>Aura Phoenix</td>\n",
       "      <td>Bigetron Era</td>\n",
       "      <td>2</td>\n",
       "      <td>9m 6s</td>\n",
       "      <td>yz, alpha, faramis, bea, mathil</td>\n",
       "      <td>karrie, gloo, yve, claude, estes</td>\n",
       "      <td>grock, fred, pharsa, bruno, lolita</td>\n",
       "      <td>wan, joy, haya, martis, barats</td>\n",
       "      <td>red</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MATCH85626e8803</td>\n",
       "      <td>MAP84a6a2dc8f24</td>\n",
       "      <td>1</td>\n",
       "      <td>WSL Season 6</td>\n",
       "      <td>https://liquipedia.net/mobilelegends/WSL/Season_6</td>\n",
       "      <td>B-Tier</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>WSL/Season 6/Regular Season</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>Aura Phoenix</td>\n",
       "      <td>Bigetron Era</td>\n",
       "      <td>2</td>\n",
       "      <td>11m 35s</td>\n",
       "      <td>fred, ling, xavier, melis, grock</td>\n",
       "      <td>karrie, haya, mathil, pharsa, lolita</td>\n",
       "      <td>joy, martis, kadita, brody, chou</td>\n",
       "      <td>wan, gloo, yve, bea, kaja</td>\n",
       "      <td>blue</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MATCHdb73da8857</td>\n",
       "      <td>MAP08d334f9487f</td>\n",
       "      <td>1</td>\n",
       "      <td>WSL Season 6</td>\n",
       "      <td>https://liquipedia.net/mobilelegends/WSL/Season_6</td>\n",
       "      <td>B-Tier</td>\n",
       "      <td>Regular Season</td>\n",
       "      <td>WSL/Season 6/Regular Season</td>\n",
       "      <td>2023-01-18</td>\n",
       "      <td>Tiger Wong Seiren</td>\n",
       "      <td>RRQ Mika</td>\n",
       "      <td>1</td>\n",
       "      <td>29m 10s</td>\n",
       "      <td>lapu, akai, valen, karrie, atlas</td>\n",
       "      <td>joy, haya, faramis, ling, lolita</td>\n",
       "      <td>bene, fred, xavier, brody, diggie</td>\n",
       "      <td>wan, yve, gloo, kaja, mathil</td>\n",
       "      <td>blue</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          match_id           map_id  game_number tournament_name  \\\n",
       "0  MATCH5ebf6583a5  MAP7466b3e7f36b            2    WSL Season 6   \n",
       "1  MATCH5ebf6583a5  MAP76eb4613f945            1    WSL Season 6   \n",
       "2  MATCH85626e8803  MAPb668e4b7d592            2    WSL Season 6   \n",
       "3  MATCH85626e8803  MAP84a6a2dc8f24            1    WSL Season 6   \n",
       "4  MATCHdb73da8857  MAP08d334f9487f            1    WSL Season 6   \n",
       "\n",
       "                                      tournament_url    tier tournament_stage  \\\n",
       "0  https://liquipedia.net/mobilelegends/WSL/Season_6  B-Tier   Regular Season   \n",
       "1  https://liquipedia.net/mobilelegends/WSL/Season_6  B-Tier   Regular Season   \n",
       "2  https://liquipedia.net/mobilelegends/WSL/Season_6  B-Tier   Regular Season   \n",
       "3  https://liquipedia.net/mobilelegends/WSL/Season_6  B-Tier   Regular Season   \n",
       "4  https://liquipedia.net/mobilelegends/WSL/Season_6  B-Tier   Regular Season   \n",
       "\n",
       "               liquipedia_path match_date              team1         team2  \\\n",
       "0  WSL/Season 6/Regular Season 2023-01-17           Foes Win   GPX Basreng   \n",
       "1  WSL/Season 6/Regular Season 2023-01-17           Foes Win   GPX Basreng   \n",
       "2  WSL/Season 6/Regular Season 2023-01-17       Aura Phoenix  Bigetron Era   \n",
       "3  WSL/Season 6/Regular Season 2023-01-17       Aura Phoenix  Bigetron Era   \n",
       "4  WSL/Season 6/Regular Season 2023-01-18  Tiger Wong Seiren      RRQ Mika   \n",
       "\n",
       "   winner duration                              team1_picks  \\\n",
       "0       2  12m 56s        hilda, bene, lylia, brody, lolita   \n",
       "1       2  13m 19s  masha, barats, cecilion, karrie, diggie   \n",
       "2       2    9m 6s          yz, alpha, faramis, bea, mathil   \n",
       "3       2  11m 35s         fred, ling, xavier, melis, grock   \n",
       "4       1  29m 10s         lapu, akai, valen, karrie, atlas   \n",
       "\n",
       "                             team1_bans                         team2_picks  \\\n",
       "0         wan, joy, gloo, estes, martis      lapu, akai, yve, karrie, atlas   \n",
       "1           wan, haya, joy, valen, fara   fred, ling, xavier, brody, lolita   \n",
       "2      karrie, gloo, yve, claude, estes  grock, fred, pharsa, bruno, lolita   \n",
       "3  karrie, haya, mathil, pharsa, lolita    joy, martis, kadita, brody, chou   \n",
       "4      joy, haya, faramis, ling, lolita   bene, fred, xavier, brody, diggie   \n",
       "\n",
       "                       team2_bans team1_side team2_side  \n",
       "0     aamon, fred, haya, ling, gs        red       blue  \n",
       "1  gloo, yve, aamon, akai, martis       blue        red  \n",
       "2  wan, joy, haya, martis, barats        red       blue  \n",
       "3       wan, gloo, yve, bea, kaja       blue        red  \n",
       "4    wan, yve, gloo, kaja, mathil       blue        red  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pulled 2023 and 2024 data. I feel that for duration I needed more data. \n",
    "#Duration feels like its more a function of playstyle and game mechanics than team rating diff.\n",
    "\n",
    "mlbb2024 = pd.read_csv(\"mlbb_2024_clean.csv\")\n",
    "side_data = pd.read_csv('mlbb_2024_side_data.csv')\n",
    "mlbb2024 = mlbb2024.merge(side_data, on='map_id', how='left')\n",
    "\n",
    "mlbb2023 = pd.read_csv(\"mlbb_2023_clean.csv\")\n",
    "side_data = pd.read_csv('mlbb_2023_side_data.csv')\n",
    "mlbb2023 = mlbb2023.merge(side_data, on='map_id', how='left')\n",
    "\n",
    "df = pd.concat([mlbb2023,mlbb2024]).reset_index(drop=True)\n",
    "df['match_date'] = pd.to_datetime(df['match_date'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c76120f-b756-40f5-87e2-8a5639466012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing time formatting\n",
    "df['duration'] = df['duration'].str.replace(r'h', ' hour ')\n",
    "df['duration'] = df['duration'].str.replace(r'm', ' min ')\n",
    "df['duration'] = df['duration'].str.replace(r's', ' sec ')\n",
    "\n",
    "#fixing ill formated times\n",
    "df.loc[610,'duration'] = '13 min 4 sec'\n",
    "df.loc[1149,'duration'] = '22 min 16 sec'\n",
    "df.loc[2645,'duration'] = '14 min 54 sec'\n",
    "df.loc[2644,'duration'] = '14 min 54 sec'\n",
    "df.loc[6497,'duration'] = '18 min 22 sec'\n",
    "df.loc[6498,'duration'] = '14 min 44 sec'\n",
    "df.loc[6499,'duration'] = '16 min 47 sec'\n",
    "df.loc[6500,'duration'] = '16 min 00 sec'\n",
    "df.loc[6501,'duration'] = '10 min 28 sec'\n",
    "df.loc[9895,'duration'] = '14 min 19 sec'\n",
    "df.loc[9988,'duration'] = '13 min 14 sec'\n",
    "df.loc[4986,'duration'] = '12 min 00 sec'\n",
    "\n",
    "df.drop(1637,inplace=True)\n",
    "\n",
    "#fixing the side data (blue/red)\n",
    "df.dropna(subset=['team1_side','team1_side'],inplace=True)\n",
    "df['team2_side'] = df['team2_side'].replace('bkye','blue')\n",
    "df['team1_side'] = df['team1_side'].replace('b;ue','blue')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#date TIME\n",
    "df['duration'] = pd.to_timedelta(df['duration']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9313eadb-acd0-48df-bf0e-dc5a9cd3bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a team mapping file to help deal with team naming issues, lots of mispellings and other problems, but I sorted it\n",
    "#I could make things better by tracking players, what I have noticed is that teams combined often which is strange.\n",
    "with open('team_name_mappings.json', 'r', encoding='utf-8') as f:\n",
    "    team_mapping = json.load(f)\n",
    "\n",
    "# Apply mapping to team columns\n",
    "df['team1'] = df['team1'].map(lambda x: team_mapping.get(x, x))\n",
    "df['team2'] = df['team2'].map(lambda x: team_mapping.get(x, x))\n",
    "\n",
    "df['team1'] = df['team1'].str.replace('fnatic onic ph','Fnatic ONIC')# missed these ;)\n",
    "df['team2'] = df['team2'].str.replace('fnatic onic ph','Fnatic ONIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e58ba542-0817-4a4a-9ae4-cc72d6a31811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hero mapping, again the heros names were not standard as well\n",
    "with open('hero_name_mappings.json', 'r', encoding='utf-8') as f:\n",
    "    hero_mapping = json.load(f)\n",
    "\n",
    "# Apply mapping to hero columns (splits comma-separated strings)\n",
    "for col in ['team1_picks', 'team1_bans', 'team2_picks', 'team2_bans']:\n",
    "    df[col] = df[col].map(lambda x: ', '.join([hero_mapping.get(h.strip().lower(), h.strip().lower()) \n",
    "                                                for h in str(x).split(',')]) if pd.notna(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c6851-5ff8-4509-8091-11bdd21d08de",
   "metadata": {},
   "source": [
    "## Team Rating Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0518da96-4b6e-4452-a989-d336f1ad015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each teams \"score\" for the map, for calculating ELO\n",
    "df['team1_map_winner'] = np.where(df['winner']==1,1,0)\n",
    "df['team2_map_winner'] = np.where(df['winner']==2,1,0)\n",
    "\n",
    "#days columm so we can loop though the data and assign elo ratings based on the last 6m of play\n",
    "df['days'] = df['match_date'].sub(df['match_date'].min()).dt.total_seconds()/60/60//24 +1\n",
    "df['team1_elo'] = np.nan\n",
    "df['team2_elo'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3d88f-593c-49ac-a47c-8b25adc2e8e8",
   "metadata": {},
   "source": [
    "I wanted to have some sort of team rating, so I used ELO, although I could have gone with a more advanced method. I am working with a custom Trueskill2 rating system now, that is really fun, but maybe better suited for prematch. Ask me about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f60fbc4-8a04-49c9-baf9-4fab9c21f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I usually begin with 3months at least of data for the first ELO's and then drop the nulls when training, but no real reason to here.\n",
    "# I use a great ratings package, for ELO here. Rankit, great book that goes along with it.\n",
    "# note I also would have used Whole History Rating (WHR), but its slower and ELO is a good baseline\n",
    "for d in df['days'].loc[4:].unique():\n",
    "    cut_off = d - 180\n",
    "    six_months = df[(df['days']<d)&(df['days']>cut_off)]\n",
    "    sm_table = Table(six_months, col = ['team1', 'team2', 'team1_map_winner', 'team2_map_winner'])\n",
    "    \n",
    "    eloRanker = EloRanker(K=89)#faster moving K value works better for esports\n",
    "    eloRanker.update(sm_table)\n",
    "    eloRank = eloRanker.leaderboard()\n",
    "    \n",
    "    rating = eloRank.set_index('name')['rating']\n",
    "    current_day_index = df[df['days'] == d].index\n",
    "    \n",
    "    team1_elos = df.loc[current_day_index, 'team1'].map(rating)\n",
    "    team2_elos = df.loc[current_day_index, 'team2'].map(rating)\n",
    "\n",
    "    df.loc[current_day_index, 'team1_elo'] = team1_elos\n",
    "    df.loc[current_day_index, 'team2_elo'] = team2_elos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624a7eec-1c63-4565-8e9f-33810597e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5-Fold Cross-Validation Scores (Accuracy) ---\n",
      "[0.61722956 0.61135585 0.6165524  0.61018609 0.625857  ]\n",
      "\n",
      "Mean CV Accuracy: 0.6162\n"
     ]
    }
   ],
   "source": [
    "#elo accuracy for fun\n",
    "df['elo_diff'] = df['team1_elo'] - df['team2_elo']\n",
    "df['map_winner'] = np.where(df['winner'] == 1,1,0)\n",
    "\n",
    "test = df.dropna()\n",
    "\n",
    "X = test[['elo_diff']]\n",
    "y = test['map_winner']\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5,              \n",
    "    scoring='accuracy'  \n",
    ")\n",
    "\n",
    "print(f\"--- 5-Fold Cross-Validation Scores (Accuracy) ---\")\n",
    "print(cv_scores)\n",
    "print(f\"\\nMean CV Accuracy: {np.mean(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1da240-5e6e-42e1-9aed-b1ff7a185316",
   "metadata": {},
   "source": [
    "Not as good as you guys ;), but could be improved with WHR or using multiple K valus and bagging. Obviously data like gold etc.. could imporve things, but its a good baseline as I said before. I think I could get 65....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88b1baf-2616-4164-ac9c-fdc5c68c298a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['match_id', 'map_id', 'game_number', 'tournament_name',\n",
       "       'tournament_url', 'tier', 'tournament_stage', 'liquipedia_path',\n",
       "       'match_date', 'team1', 'team2', 'winner', 'duration', 'team1_picks',\n",
       "       'team1_bans', 'team2_picks', 'team2_bans', 'team1_side', 'team2_side',\n",
       "       'team1_map_winner', 'team2_map_winner', 'days', 'team1_elo',\n",
       "       'team2_elo', 'elo_diff', 'map_winner'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61eaa4-e3ed-4565-a97e-a5424f3a9312",
   "metadata": {},
   "source": [
    "## Further FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c847023e-63b4-4cdc-8a6d-f9198386d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing as the other notebook, calculating population data, two windows this time\n",
    "\n",
    "historical_stats_dict = {}\n",
    "window_list = [90, 180]\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    current_date = row['match_date']\n",
    "    current_match_id = row['match_id']\n",
    "    \n",
    "    \n",
    "    current_map_stats = {}\n",
    "    \n",
    "    # getting 90 and 180 days this time\n",
    "    for window_days in window_list:\n",
    "        \n",
    "        cutoff_date = current_date - pd.DateOffset(days=window_days)\n",
    "        \n",
    "        \n",
    "        historical_data = df[\n",
    "            (df['match_date'] < current_date) &      # Data is in the past\n",
    "            (df['match_date'] >= cutoff_date) &     # Data is within the window\n",
    "            (df['match_id'] != current_match_id)  # EXCLUDE other maps from the current match\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        stats_duration = historical_data['duration'].describe()\n",
    "            \n",
    "        #\n",
    "        # This adds the stats for the current window to the map's dictionary\n",
    "        current_map_stats.update({\n",
    "            f'hist_dur_{window_days}D_mean': stats_duration['mean'],\n",
    "            f'hist_dur_{window_days}D_std': stats_duration['std'],\n",
    "            f'hist_dur_{window_days}D_min': stats_duration['min'],\n",
    "            f'hist_dur_{window_days}D_max': stats_duration['max'],\n",
    "            f'hist_dur_{window_days}D_q25': stats_duration['25%'],\n",
    "            f'hist_dur_{window_days}D_q50': stats_duration['50%'],\n",
    "            f'hist_dur_{window_days}D_q75': stats_duration['75%']\n",
    "        })\n",
    "\n",
    "    #Store all collected stats for this map_id\n",
    "    historical_stats_dict[row['map_id']] = current_map_stats\n",
    "\n",
    "\n",
    "# This part is the same as before.\n",
    "features_df = pd.DataFrame.from_dict(historical_stats_dict, orient='index')\n",
    "features_df = features_df.reset_index().rename(columns={'index': 'map_id'})\n",
    "df = df.merge(features_df, on='map_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ce10c89-2308-4939-b113-67e50706a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing as the other notebook, calculating team data, two windows this time\n",
    "\n",
    "historical_stats_dict = {}\n",
    "window_list = [90, 180] \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    # Get current match info\n",
    "    current_date = row['match_date']\n",
    "    current_match_id = row['match_id']\n",
    "    team1_name = row['team1']\n",
    "    team2_name = row['team2']\n",
    "    \n",
    "    \n",
    "    current_map_stats = {}\n",
    "    \n",
    "    \n",
    "    for window_days in window_list:\n",
    "        \n",
    "        \n",
    "        cutoff_date = current_date - pd.DateOffset(days=window_days)\n",
    "        \n",
    "        \n",
    "        # This gets all maps played by any team in the window\n",
    "        # It excludes maps from the current match to prevent data leakage\n",
    "        historical_data_all = df[\n",
    "            (df['match_date'] < current_date) &      # Data is in the past\n",
    "            (df['match_date'] >= cutoff_date) &     # Data is within the window\n",
    "            (df['match_id'] != current_match_id) \n",
    "        ]\n",
    "        \n",
    "        \n",
    "        # Find all historical maps where 'team1' played\n",
    "        team1_hist_slice = historical_data_all[\n",
    "            (historical_data_all['team1'] == team1_name) | \n",
    "            (historical_data_all['team2'] == team1_name)\n",
    "        ]\n",
    "        \n",
    "        # Find all historical maps where 'team2' played\n",
    "        team2_hist_slice = historical_data_all[\n",
    "            (historical_data_all['team1'] == team2_name) | \n",
    "            (historical_data_all['team2'] == team2_name)\n",
    "        ]\n",
    "        \n",
    "        #inputing our stats to the current deal\n",
    "        current_map_stats[f'team1_hist_dur_{window_days}D'] = team1_hist_slice['duration'].mean()\n",
    "        current_map_stats[f'team2_hist_dur_{window_days}D'] = team2_hist_slice['duration'].mean()\n",
    "\n",
    "\n",
    "    # After loop, add this map's stats to the main dictionary\n",
    "    historical_stats_dict[row['map_id']] = current_map_stats\n",
    "\n",
    "\n",
    "\n",
    "# Convert the dictionary of stats into a DataFrame\n",
    "features_df = pd.DataFrame.from_dict(historical_stats_dict, orient='index')\n",
    "\n",
    "# Make 'map_id' (which is the dict key) a regular column\n",
    "features_df = features_df.reset_index().rename(columns={'index': 'map_id'})\n",
    "\n",
    "# Merge the new historical features back into your main df\n",
    "df = df.merge(features_df, on='map_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95003f51-5cb7-4f50-ac88-bde58ee3a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature importance, getting ready for RF OOF preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c084ce-709b-49cc-8749-9c296cb31c2b",
   "metadata": {},
   "source": [
    "I never use this but why not see feature importance for this data we are getting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "776d269d-972d-4314-a445-a960df052ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Random Forest Feature Importances ---\n",
      "sum_90D                0.122989\n",
      "sum_180D               0.122574\n",
      "team2_hist_dur_180D    0.098420\n",
      "team1_hist_dur_90D     0.097620\n",
      "team2_hist_dur_90D     0.094596\n",
      "team1_hist_dur_180D    0.093926\n",
      "hist_dur_90D_std       0.055437\n",
      "hist_dur_180D_std      0.052427\n",
      "hist_dur_180D_q75      0.035606\n",
      "hist_dur_90D_mean      0.035084\n",
      "hist_dur_180D_mean     0.032849\n",
      "hist_dur_90D_q75       0.032800\n",
      "hist_dur_90D_q25       0.029738\n",
      "hist_dur_90D_q50       0.025863\n",
      "hist_dur_180D_q50      0.023433\n",
      "hist_dur_180D_q25      0.020054\n",
      "hist_dur_90D_max       0.008319\n",
      "hist_dur_90D_min       0.007989\n",
      "hist_dur_180D_min      0.005423\n",
      "hist_dur_180D_max      0.004853\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['sum_90D'] = df['team1_hist_dur_90D'] + df['team2_hist_dur_90D']#creating our sum feat from the other notebook\n",
    "df['sum_180D'] = df['team1_hist_dur_180D'] + df['team2_hist_dur_180D']#creating our sum feat from the other notebook\n",
    "\n",
    "feature_cols = list(df.columns[26:]) #all the feats we just generated.\n",
    "all_cols_to_check = feature_cols + ['duration'] #cols to make sure are clean, although not really needed for RF\n",
    "\n",
    "test = df.dropna(subset=all_cols_to_check).copy()\n",
    "\n",
    "X = test[feature_cols]\n",
    "y = test['duration']\n",
    "\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,  \n",
    "    random_state=99, #red ballons  \n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "feature_scores = pd.Series(importances, index=feature_cols)\n",
    "\n",
    "feature_scores = feature_scores.sort_values(ascending=False)\n",
    "\n",
    "#for you guys\n",
    "print(\"--- Random Forest Feature Importances ---\")\n",
    "print(feature_scores) # these scorea are about interaction, but its clear I think that the sum of rolling duration is decent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23465616-3564-402d-a216-81aa828ef48a",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5227ef6-f4fd-4890-823e-18a7c6eece1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores (RMSE): [-279.47514643 -265.91317409 -270.48578163 -303.69355137 -273.85870685]\n",
      "Mean CV RMSE: -278.6853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#figure I'd use this as a baseline to work from\n",
    "features = ['sum_90D']\n",
    "model = LinearRegression()\n",
    "evaluate_model(model=model, features=features, data=df, target='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf6d1c20-22c1-4e8a-bfb7-ada0fc289fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hist_dur_90D_mean',\n",
       " 'hist_dur_90D_std',\n",
       " 'hist_dur_90D_min',\n",
       " 'hist_dur_90D_max',\n",
       " 'hist_dur_90D_q25',\n",
       " 'hist_dur_90D_q50',\n",
       " 'hist_dur_90D_q75',\n",
       " 'hist_dur_180D_mean',\n",
       " 'hist_dur_180D_std',\n",
       " 'hist_dur_180D_min',\n",
       " 'hist_dur_180D_max',\n",
       " 'hist_dur_180D_q25',\n",
       " 'hist_dur_180D_q50',\n",
       " 'hist_dur_180D_q75',\n",
       " 'team1_hist_dur_90D',\n",
       " 'team2_hist_dur_90D',\n",
       " 'team1_hist_dur_180D',\n",
       " 'team2_hist_dur_180D',\n",
       " 'sum_90D']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1edc8-838e-40e3-b4c4-04e5d647acaa",
   "metadata": {},
   "source": [
    "## Creation of RF OOF Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a46c52c0-a9a2-400f-90ba-2b7d60cc6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the some RF preds, in future model can use XGboost or some other model\n",
    "X = df[feature_cols[:-1]]#all columns except sum\n",
    "y = df['duration']\n",
    "\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    random_state=99,  \n",
    ")\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=99)\n",
    "\n",
    "oof_predictions = cross_val_predict(\n",
    "    estimator=model_rf,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    ")\n",
    "\n",
    "df['rf_pred'] = oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "642ef79a-bae5-4b5a-aa78-192ac349f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 2: Stacked LR (Sum + RF OOF + ELO) ---\n",
      "CV Scores (RMSE): [-277.59300092 -265.2461245  -269.61551957 -302.7984314  -272.87817541]\n",
      "Mean CV RMSE: -277.6263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Iteration 2: Stacked LR (Sum + RF OOF + ELO) ---\")\n",
    "features_v2 = ['sum_90D','rf_pred','elo_diff']\n",
    "model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "# 'test' was created in the cell above, which is correct\n",
    "evaluate_model(model=model, features=features_v2, data=df, target='duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0b221-c9b6-4617-86b5-9028ec75ca51",
   "metadata": {},
   "source": [
    "#### -278.6853 --> -277.5179"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e123be2-bb59-4769-adee-ef40f29dde10",
   "metadata": {},
   "source": [
    "## Embedding Model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This is my first time working with this type of model, although it has been on my self and I have been planning to use it for my personal modeling. I like to try models quick first, if I see potential than I dive deeper, unlocking complexity and new potential. I built this in Cursor with the help of AI, I treat it like working with more technical subordinate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The architecture and ideas are mine, but I use it to prototype quickly. If I were to continue to work on this model, I would dig deeper and read long form research on this type of model, to use it better.\n",
    "\n",
    "This is all commented by AI, however I have looked over every line myself as well and understand it fully. Feel free to ask me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae64420e-f2e9-460f-9b6f-c81c6429156c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique teams (vocab size): 740\n",
      "Total unique heroes (vocab size): 133\n",
      "Total unique sides (vocab size): 3\n"
     ]
    }
   ],
   "source": [
    "# 1. Concatenate all hero string columns\n",
    "all_heroes_raw = pd.concat([\n",
    "    df['team1_picks'], \n",
    "    df['team2_picks'], \n",
    "    df['team1_bans'], \n",
    "    df['team2_bans']\n",
    "])\n",
    "\n",
    "# 2. Split strings into lists, explode, and CLEAN\n",
    "# This is the new, critical part\n",
    "all_heroes = all_heroes_raw \\\n",
    "    .str.split(',') \\\n",
    "    .explode() \\\n",
    "    .str.strip() # <-- This cleans \" bene\" and \"lolita\\n\" into \"bene\" and \"lolita\"\n",
    "\n",
    "# 3. Create the \"lookup maps\" (dictionaries)\n",
    "# We add +1 to reserve 0 for \"padding\" or \"unknown\"\n",
    "team_map = {name: i+1 for i, name in enumerate(pd.concat([df['team1'], df['team2']]).unique())}\n",
    "hero_map = {name: i+1 for i, name in enumerate(all_heroes.unique())} # Now this will be correct\n",
    "side_map = {'blue': 1, 'red': 2} # 0 will be our padding/unknown\n",
    "\n",
    "# --- 4. Apply the maps to your DataFrame ---\n",
    "\n",
    "# Convert team names to their new IDs\n",
    "df['team1_id'] = df['team1'].map(team_map)\n",
    "df['team2_id'] = df['team2'].map(team_map)\n",
    "\n",
    "# Convert side to its ID (assuming these column names)\n",
    "df['team1_side_id'] = df['team1_side'].map(side_map).fillna(0)\n",
    "df['team2_side_id'] = df['team2_side'].map(side_map).fillna(0)\n",
    "\n",
    "# Helper function to turn a list of hero names into a list of hero IDs\n",
    "def map_hero_list(hero_string):\n",
    "    if not isinstance(hero_string, str):\n",
    "        return [] # Handle NaNs\n",
    "    \n",
    "    # Split the string and clean it, just like we did above\n",
    "    hero_list = [name.strip() for name in hero_string.split(',')]\n",
    "    \n",
    "    # Map to IDs\n",
    "    return [hero_map.get(hero, 0) for hero in hero_list] # 0 = unknown hero\n",
    "\n",
    "df['team1_picks_ids'] = df['team1_picks'].apply(map_hero_list)\n",
    "df['team2_picks_ids'] = df['team2_picks'].apply(map_hero_list)\n",
    "df['team1_bans_ids'] = df['team1_bans'].apply(map_hero_list)\n",
    "df['team2_bans_ids'] = df['team2_bans'].apply(map_hero_list)\n",
    "\n",
    "# 5. Store the \"vocab size\" (total count)\n",
    "n_teams = len(team_map) + 1 \n",
    "n_heroes = len(hero_map) + 1 # This number should now be reasonable (e.g., 120-130)\n",
    "n_sides = 3 \n",
    "\n",
    "print(f\"Total unique teams (vocab size): {n_teams}\")\n",
    "print(f\"Total unique heroes (vocab size): {n_heroes}\") # <-- Check this number!\n",
    "print(f\"Total unique sides (vocab size): {n_sides}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d445084a-079a-4d79-8b5e-df2abaa6f5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "------------------------------\n",
      "--- FOLD 1/5 ---\n",
      "------------------------------\n",
      "Epoch 01 | Train Loss (MSE): 733686.7885 | Val MAE: 237.7484 | Val RMSE: 300.6569\n",
      "Epoch 02 | Train Loss (MSE): 123591.4273 | Val MAE: 218.8891 | Val RMSE: 288.5551\n",
      "Epoch 03 | Train Loss (MSE): 119817.9876 | Val MAE: 214.6337 | Val RMSE: 283.5437\n",
      "Epoch 04 | Train Loss (MSE): 118679.0078 | Val MAE: 212.9408 | Val RMSE: 279.4671\n",
      "Epoch 05 | Train Loss (MSE): 115560.2436 | Val MAE: 211.1536 | Val RMSE: 277.5276\n",
      "Epoch 06 | Train Loss (MSE): 110819.6746 | Val MAE: 209.2192 | Val RMSE: 276.5268\n",
      "Epoch 07 | Train Loss (MSE): 112725.2691 | Val MAE: 210.7617 | Val RMSE: 274.2731\n",
      "Epoch 08 | Train Loss (MSE): 110351.5127 | Val MAE: 207.7843 | Val RMSE: 274.3021\n",
      "Epoch 09 | Train Loss (MSE): 108018.8015 | Val MAE: 209.1261 | Val RMSE: 272.8521\n",
      "Epoch 10 | Train Loss (MSE): 107423.1145 | Val MAE: 210.2384 | Val RMSE: 272.2852\n",
      "Best Val MAE for Fold 1: 207.7843\n",
      "------------------------------\n",
      "--- FOLD 2/5 ---\n",
      "------------------------------\n",
      "Epoch 01 | Train Loss (MSE): 715334.9787 | Val MAE: 251.2120 | Val RMSE: 342.9703\n",
      "Epoch 02 | Train Loss (MSE): 121736.8689 | Val MAE: 230.9323 | Val RMSE: 325.7212\n",
      "Epoch 03 | Train Loss (MSE): 112140.2312 | Val MAE: 227.7297 | Val RMSE: 318.2697\n",
      "Epoch 04 | Train Loss (MSE): 109067.5041 | Val MAE: 223.7714 | Val RMSE: 315.9289\n",
      "Epoch 05 | Train Loss (MSE): 106809.5083 | Val MAE: 221.8753 | Val RMSE: 313.7931\n",
      "Epoch 06 | Train Loss (MSE): 106433.1907 | Val MAE: 220.8931 | Val RMSE: 311.4287\n",
      "Epoch 07 | Train Loss (MSE): 104787.7774 | Val MAE: 220.0588 | Val RMSE: 309.5149\n",
      "Epoch 08 | Train Loss (MSE): 104181.8077 | Val MAE: 218.2047 | Val RMSE: 309.2856\n",
      "Epoch 09 | Train Loss (MSE): 102957.0448 | Val MAE: 216.4709 | Val RMSE: 311.3602\n",
      "Epoch 10 | Train Loss (MSE): 102254.6476 | Val MAE: 219.2096 | Val RMSE: 306.8430\n",
      "Best Val MAE for Fold 2: 216.4709\n",
      "------------------------------\n",
      "--- FOLD 3/5 ---\n",
      "------------------------------\n",
      "Epoch 01 | Train Loss (MSE): 757695.0707 | Val MAE: 244.7657 | Val RMSE: 316.5987\n",
      "Epoch 02 | Train Loss (MSE): 126694.2812 | Val MAE: 229.9236 | Val RMSE: 306.0863\n",
      "Epoch 03 | Train Loss (MSE): 120485.5485 | Val MAE: 224.2130 | Val RMSE: 297.9520\n",
      "Epoch 04 | Train Loss (MSE): 114098.3042 | Val MAE: 219.7467 | Val RMSE: 294.0519\n",
      "Epoch 05 | Train Loss (MSE): 115361.4119 | Val MAE: 216.5882 | Val RMSE: 291.4735\n",
      "Epoch 06 | Train Loss (MSE): 111784.2552 | Val MAE: 214.8012 | Val RMSE: 288.8958\n",
      "Epoch 07 | Train Loss (MSE): 111314.1715 | Val MAE: 214.7282 | Val RMSE: 284.4752\n",
      "Epoch 08 | Train Loss (MSE): 110268.3688 | Val MAE: 212.0141 | Val RMSE: 285.7759\n",
      "Epoch 09 | Train Loss (MSE): 107483.3183 | Val MAE: 211.8025 | Val RMSE: 283.9033\n",
      "Epoch 10 | Train Loss (MSE): 107735.5735 | Val MAE: 210.5191 | Val RMSE: 285.6221\n",
      "Best Val MAE for Fold 3: 210.5191\n",
      "------------------------------\n",
      "--- FOLD 4/5 ---\n",
      "------------------------------\n",
      "Epoch 01 | Train Loss (MSE): 736798.1894 | Val MAE: 234.7525 | Val RMSE: 296.8185\n",
      "Epoch 02 | Train Loss (MSE): 122463.5692 | Val MAE: 218.4906 | Val RMSE: 287.2740\n",
      "Epoch 03 | Train Loss (MSE): 120726.6468 | Val MAE: 215.1166 | Val RMSE: 282.0775\n",
      "Epoch 04 | Train Loss (MSE): 115706.2017 | Val MAE: 212.3257 | Val RMSE: 285.9049\n",
      "Epoch 05 | Train Loss (MSE): 115355.5028 | Val MAE: 210.8728 | Val RMSE: 277.5454\n",
      "Epoch 06 | Train Loss (MSE): 113960.9072 | Val MAE: 209.8442 | Val RMSE: 275.3207\n",
      "Epoch 07 | Train Loss (MSE): 111763.7386 | Val MAE: 208.0162 | Val RMSE: 278.5742\n",
      "Epoch 08 | Train Loss (MSE): 109221.2582 | Val MAE: 209.7912 | Val RMSE: 272.3203\n",
      "Epoch 09 | Train Loss (MSE): 111387.5563 | Val MAE: 208.5729 | Val RMSE: 271.8353\n",
      "Epoch 10 | Train Loss (MSE): 109984.7938 | Val MAE: 210.6165 | Val RMSE: 270.8673\n",
      "Best Val MAE for Fold 4: 208.0162\n",
      "------------------------------\n",
      "--- FOLD 5/5 ---\n",
      "------------------------------\n",
      "Epoch 01 | Train Loss (MSE): 720892.6732 | Val MAE: 253.4770 | Val RMSE: 327.5478\n",
      "Epoch 02 | Train Loss (MSE): 122129.3807 | Val MAE: 231.2486 | Val RMSE: 306.4369\n",
      "Epoch 03 | Train Loss (MSE): 112885.0381 | Val MAE: 227.6722 | Val RMSE: 298.0877\n",
      "Epoch 04 | Train Loss (MSE): 109183.0111 | Val MAE: 221.2087 | Val RMSE: 294.9751\n",
      "Epoch 05 | Train Loss (MSE): 107232.4898 | Val MAE: 218.6444 | Val RMSE: 291.9246\n",
      "Epoch 06 | Train Loss (MSE): 104364.1496 | Val MAE: 220.5641 | Val RMSE: 288.2003\n",
      "Epoch 07 | Train Loss (MSE): 104824.8268 | Val MAE: 217.7146 | Val RMSE: 286.8392\n",
      "Epoch 08 | Train Loss (MSE): 103423.4358 | Val MAE: 216.8406 | Val RMSE: 286.1522\n",
      "Epoch 09 | Train Loss (MSE): 104948.2872 | Val MAE: 217.3347 | Val RMSE: 285.0083\n",
      "Epoch 10 | Train Loss (MSE): 101035.7856 | Val MAE: 212.9634 | Val RMSE: 286.3383\n",
      "Best Val MAE for Fold 5: 212.9634\n",
      "\n",
      "==============================\n",
      "TOTAL OOF MAE: 219.99137\n",
      "TOTAL OOF RMSE: 298.77116\n",
      "==============================\n",
      "\n",
      "OOF duration predictions saved to a new Series 'oof_pred_series'.\n",
      "0     895.266296\n",
      "1     964.818604\n",
      "2     933.589172\n",
      "3     995.115356\n",
      "4    1136.075684\n",
      "Name: oof_pred_simple_model_duration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "N_SPLITS = 5\n",
    "N_EPOCHS = 10 \n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "TARGET_COLUMN = 'duration' \n",
    "\n",
    "# --- INITIALIZATION ---\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "oof_preds = np.zeros(len(df))\n",
    "oof_indices = []\n",
    "\n",
    "# --- 3. Start the K-Fold Loop ---\n",
    "groups = df['match_id']\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=groups)):\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # 1. Create DataLoaders\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "    \n",
    "    train_dataset = MatchDataset(train_df, target_column=TARGET_COLUMN)\n",
    "    val_dataset = MatchDataset(val_df, target_column=TARGET_COLUMN)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = SimpleModel(n_teams, n_heroes, n_sides).to(DEVICE)\n",
    "    \n",
    "    # 3. Define Loss Function and Optimizer\n",
    "    criterion = nn.MSELoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    best_val_mae = float('inf') # Set to infinity so any error is better\n",
    "    best_model_state = None\n",
    "\n",
    "    # 4. Start Training Loop for this model\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = train_loop(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        \n",
    "        val_preds, val_targets = get_predictions(model, val_loader, DEVICE)\n",
    "        \n",
    "        #Calculate regression metrics ---\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:02d} | Train Loss (MSE): {train_loss:.4f} | Val MAE: {val_mae:.4f} | Val RMSE: {val_rmse:.4f}\")\n",
    "        \n",
    "        #Save model with the lowest MAE\n",
    "        if val_mae < best_val_mae:\n",
    "            best_val_mae = val_mae\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # 5. Get OOF predictions from the best model\n",
    "    print(f\"Best Val MAE for Fold {fold+1}: {best_val_mae:.4f}\")\n",
    "    model.load_state_dict(best_model_state) \n",
    "    \n",
    "    oof_preds_fold, _ = get_predictions(model, val_loader, DEVICE)\n",
    "    \n",
    "    # 6. Store OOF predictions\n",
    "    oof_preds[val_idx] = oof_preds_fold\n",
    "    oof_indices.extend(val_idx)\n",
    "\n",
    "#Final OOF Score and Saving to Series ---\n",
    "\n",
    "#Calculate total OOF regression score ---\n",
    "total_oof_targets = df[TARGET_COLUMN].iloc[oof_indices]\n",
    "total_oof_mae = mean_absolute_error(total_oof_targets, oof_preds)\n",
    "total_oof_rmse = np.sqrt(mean_squared_error(total_oof_targets, oof_preds))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(f\"TOTAL OOF MAE: {total_oof_mae:.5f}\")\n",
    "print(f\"TOTAL OOF RMSE: {total_oof_rmse:.5f}\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create a new pandas Series for the OOF predictions\n",
    "oof_pred_series = pd.Series(\n",
    "    oof_preds,\n",
    "    index=df.index,\n",
    "    name='oof_pred_simple_model_duration' # Changed name\n",
    ")\n",
    "\n",
    "print(\"\\nOOF duration predictions saved to a new Series 'oof_pred_series'.\")\n",
    "print(oof_pred_series.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5b7af33-8da9-49d1-8b6d-48ccb78ebb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c3764a7-4583-44a8-a6d9-2a91900bfc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 3: Stacked LR (All features + Simple Embed OOF) ---\n",
      "CV Scores (RMSE): [-277.05464956 -264.22660597 -269.33851168 -301.85784724 -271.37134773]\n",
      "Mean CV RMSE: -276.7698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['embed_oof'] = oof_pred_series\n",
    "features_v3 = ['sum_90D', 'rf_pred', 'elo_diff', 'embed_oof']\n",
    "model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "# 'test' has the 'embed_oof' column from the cell above\n",
    "evaluate_model(model=model, features=features_v3, data=df, target='duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a7ade6-eb20-41f5-861d-efcd7bfaf705",
   "metadata": {},
   "source": [
    "#### This is the same model, but now it is taking into account the pick pick order of the draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08f8b08c-4ef3-4797-bdc1-b8e39bc585d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique teams: 740\n",
      "Total unique heroes: 133\n",
      "Total unique sides: 3\n",
      "Step 1 Complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Clean and Explode Hero Strings ---\n",
    "all_heroes_raw = pd.concat([\n",
    "    df['team1_picks'], \n",
    "    df['team2_picks'], \n",
    "    df['team1_bans'], \n",
    "    df['team2_bans']\n",
    "])\n",
    "\n",
    "all_heroes = all_heroes_raw \\\n",
    "    .astype(str) \\\n",
    "    .str.split(',') \\\n",
    "    .explode() \\\n",
    "    .str.strip() \\\n",
    "    .unique()\n",
    "\n",
    "all_teams = pd.concat([df['team1'], df['team2']]).unique()\n",
    "\n",
    "# --- 3. Create Lookup Maps (Vocabularies) ---\n",
    "team_map = {name: i+1 for i, name in enumerate(all_teams)}\n",
    "hero_map = {name: i+1 for i, name in enumerate(all_heroes)}\n",
    "side_map = {'blue': 1, 'red': 2}\n",
    "\n",
    "# --- 4. Apply Maps to Create ID Columns ---\n",
    "df['team1_id'] = df['team1'].map(team_map).fillna(0)\n",
    "df['team2_id'] = df['team2'].map(team_map).fillna(0)\n",
    "df['team1_side_id'] = df['team1_side'].map(side_map).fillna(0)\n",
    "df['team2_side_id'] = df['team2_side'].map(side_map).fillna(0)\n",
    "\n",
    "# Helper function to map hero lists\n",
    "def map_hero_list(hero_string):\n",
    "    if not isinstance(hero_string, str):\n",
    "        return []\n",
    "    hero_list = [name.strip() for name in hero_string.split(',')]\n",
    "    return [hero_map.get(hero, 0) for hero in hero_list]\n",
    "\n",
    "df['team1_picks_ids'] = df['team1_picks'].apply(map_hero_list)\n",
    "df['team2_picks_ids'] = df['team2_picks'].apply(map_hero_list)\n",
    "df['team1_bans_ids'] = df['team1_bans'].apply(map_hero_list)\n",
    "df['team2_bans_ids'] = df['team2_bans'].apply(map_hero_list)\n",
    "\n",
    "# --- 5. Store Vocab Sizes ---\n",
    "n_teams = len(team_map) + 1\n",
    "n_heroes = len(hero_map) + 1\n",
    "n_sides = 3 # (blue, red) + (padding_idx 0)\n",
    "\n",
    "print(f\"Total unique teams: {n_teams}\")\n",
    "print(f\"Total unique heroes: {n_heroes}\")\n",
    "print(f\"Total unique sides: {n_sides}\")\n",
    "print(\"Step 1 Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99b9c39e-04ac-4904-99de-575477b1c269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Step 2: Defining Dataset ---\n",
      "Step 2 Complete (MatchDataset defined).\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Running Step 2: Defining Dataset ---\")\n",
    "\n",
    "# --- 2. Define the Dataset Class ---\n",
    "class MatchDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_column):\n",
    "        self.df = dataframe\n",
    "        \n",
    "        # Single-value features\n",
    "        self.team1_ids = self.df['team1_id'].values\n",
    "        self.team2_ids = self.df['team2_id'].values\n",
    "        self.team1_side_ids = self.df['team1_side_id'].values\n",
    "        self.team2_side_ids = self.df['team2_side_id'].values\n",
    "        \n",
    "        \n",
    "        # List-based features\n",
    "        self.t1_picks = self.df['team1_picks_ids'].values\n",
    "        self.t2_picks = self.df['team2_picks_ids'].values\n",
    "        self.t1_bans = self.df['team1_bans_ids'].values\n",
    "        self.t2_bans = self.df['team2_bans_ids'].values\n",
    "        \n",
    "        self.target = self.df[target_column].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pad_hero_list(self, hero_list):\n",
    "        \"\"\"Pads or truncates a hero list to MAX_LEN.\"\"\"\n",
    "        padded_list = hero_list + [PADDING_VALUE] * (HERO_LIST_MAX_LEN - len(hero_list))\n",
    "        return padded_list[:HERO_LIST_MAX_LEN]\n",
    "\n",
    "    def _get_positions(self, hero_list):\n",
    "        \"\"\"Creates a position list [1, 2, 3, ...] for non-padded heroes.\"\"\"\n",
    "        positions = list(range(1, HERO_LIST_MAX_LEN + 1))\n",
    "        # Set position to 0 if the hero_id is 0 (padded)\n",
    "        for i, hero_id in enumerate(hero_list):\n",
    "            if hero_id == PADDING_VALUE:\n",
    "                positions[i] = PADDING_VALUE\n",
    "        return positions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get hero ID lists\n",
    "        t1_picks_list = self._pad_hero_list(self.t1_picks[idx])\n",
    "        t2_picks_list = self._pad_hero_list(self.t2_picks[idx])\n",
    "        t1_bans_list = self._pad_hero_list(self.t1_bans[idx])\n",
    "        t2_bans_list = self._pad_hero_list(self.t2_bans[idx])\n",
    "        \n",
    "        # Create corresponding position lists\n",
    "        t1_picks_pos = self._get_positions(t1_picks_list)\n",
    "        t2_picks_pos = self._get_positions(t2_picks_list)\n",
    "        t1_bans_pos = self._get_positions(t1_bans_list)\n",
    "        t2_bans_pos = self._get_positions(t2_bans_list)\n",
    "        \n",
    "        features = {\n",
    "            # Single IDs\n",
    "            'team1_id': torch.tensor(self.team1_ids[idx], dtype=torch.long),\n",
    "            'team2_id': torch.tensor(self.team2_ids[idx], dtype=torch.long),\n",
    "            'team1_side_id': torch.tensor(self.team1_side_ids[idx], dtype=torch.long),\n",
    "            'team2_side_id': torch.tensor(self.team2_side_ids[idx], dtype=torch.long),\n",
    "            \n",
    "            \n",
    "            # Hero IDs\n",
    "            't1_picks': torch.tensor(t1_picks_list, dtype=torch.long),\n",
    "            't2_picks': torch.tensor(t2_picks_list, dtype=torch.long),\n",
    "            't1_bans': torch.tensor(t1_bans_list, dtype=torch.long),\n",
    "            't2_bans': torch.tensor(t2_bans_list, dtype=torch.long),\n",
    "            \n",
    "            # Position IDs\n",
    "            't1_picks_pos': torch.tensor(t1_picks_pos, dtype=torch.long),\n",
    "            't2_picks_pos': torch.tensor(t2_picks_pos, dtype=torch.long),\n",
    "            't1_bans_pos': torch.tensor(t1_bans_pos, dtype=torch.long),\n",
    "            't2_bans_pos': torch.tensor(t2_bans_pos, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        target = torch.tensor(self.target[idx], dtype=torch.float)\n",
    "        return features, target\n",
    "\n",
    "print(\"Step 2 Complete (MatchDataset defined).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db9b7494-4015-4b81-94ad-47712ae50906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Step 4: Starting K-Fold Training ---\n",
      "------------------------------\n",
      "--- FOLD 1/5 ---\n",
      "------------------------------\n",
      "  [Model Init] n_teams: 740\n",
      "  [Model Init] n_heroes: 133\n",
      "  [Model Init] n_sides: 3\n",
      "  [Model Init] n_positions: 6\n",
      "Epoch 01 | Train Loss (MSE): 598852.4436 | Val MAE: 210.3272\n",
      "Epoch 02 | Train Loss (MSE): 114309.2963 | Val MAE: 216.5254\n",
      "Epoch 03 | Train Loss (MSE): 113519.3875 | Val MAE: 209.4435\n",
      "Epoch 04 | Train Loss (MSE): 111868.2330 | Val MAE: 209.2863\n",
      "Epoch 05 | Train Loss (MSE): 110472.8319 | Val MAE: 206.9114\n",
      "Epoch 06 | Train Loss (MSE): 109491.9091 | Val MAE: 204.7323\n",
      "Epoch 07 | Train Loss (MSE): 109333.5511 | Val MAE: 206.5630\n",
      "Epoch 08 | Train Loss (MSE): 107673.7203 | Val MAE: 206.9942\n",
      "Epoch 09 | Train Loss (MSE): 105930.7020 | Val MAE: 206.8546\n",
      "Epoch 10 | Train Loss (MSE): 106418.9358 | Val MAE: 204.0603\n",
      "Best Val MAE for Fold 1: 204.0603\n",
      "------------------------------\n",
      "--- FOLD 2/5 ---\n",
      "------------------------------\n",
      "  [Model Init] n_teams: 740\n",
      "  [Model Init] n_heroes: 133\n",
      "  [Model Init] n_sides: 3\n",
      "  [Model Init] n_positions: 6\n",
      "Epoch 01 | Train Loss (MSE): 643747.0865 | Val MAE: 221.2910\n",
      "Epoch 02 | Train Loss (MSE): 115892.8196 | Val MAE: 217.7623\n",
      "Epoch 03 | Train Loss (MSE): 115315.6761 | Val MAE: 215.1002\n",
      "Epoch 04 | Train Loss (MSE): 110878.1923 | Val MAE: 213.7180\n",
      "Epoch 05 | Train Loss (MSE): 109576.7774 | Val MAE: 213.5404\n",
      "Epoch 06 | Train Loss (MSE): 108337.8283 | Val MAE: 214.1014\n",
      "Epoch 07 | Train Loss (MSE): 107699.6493 | Val MAE: 213.7340\n",
      "Epoch 08 | Train Loss (MSE): 104810.1674 | Val MAE: 211.0265\n",
      "Epoch 09 | Train Loss (MSE): 104311.8512 | Val MAE: 211.4481\n",
      "Epoch 10 | Train Loss (MSE): 104638.0119 | Val MAE: 214.1769\n",
      "Best Val MAE for Fold 2: 211.0265\n",
      "------------------------------\n",
      "--- FOLD 3/5 ---\n",
      "------------------------------\n",
      "  [Model Init] n_teams: 740\n",
      "  [Model Init] n_heroes: 133\n",
      "  [Model Init] n_sides: 3\n",
      "  [Model Init] n_positions: 6\n",
      "Epoch 01 | Train Loss (MSE): 648817.7872 | Val MAE: 221.1346\n",
      "Epoch 02 | Train Loss (MSE): 119480.4189 | Val MAE: 214.6543\n",
      "Epoch 03 | Train Loss (MSE): 114356.7195 | Val MAE: 214.0881\n",
      "Epoch 04 | Train Loss (MSE): 112600.1310 | Val MAE: 211.5200\n",
      "Epoch 05 | Train Loss (MSE): 112962.3979 | Val MAE: 210.9554\n",
      "Epoch 06 | Train Loss (MSE): 110018.4215 | Val MAE: 210.4492\n",
      "Epoch 07 | Train Loss (MSE): 105936.0447 | Val MAE: 210.8134\n",
      "Epoch 08 | Train Loss (MSE): 103825.7744 | Val MAE: 210.4448\n",
      "Epoch 09 | Train Loss (MSE): 105388.8653 | Val MAE: 209.0415\n",
      "Epoch 10 | Train Loss (MSE): 105674.6407 | Val MAE: 208.7378\n",
      "Best Val MAE for Fold 3: 208.7378\n",
      "------------------------------\n",
      "--- FOLD 4/5 ---\n",
      "------------------------------\n",
      "  [Model Init] n_teams: 740\n",
      "  [Model Init] n_heroes: 133\n",
      "  [Model Init] n_sides: 3\n",
      "  [Model Init] n_positions: 6\n",
      "Epoch 01 | Train Loss (MSE): 572037.7415 | Val MAE: 208.0961\n",
      "Epoch 02 | Train Loss (MSE): 113541.0556 | Val MAE: 207.1269\n",
      "Epoch 03 | Train Loss (MSE): 112870.8740 | Val MAE: 206.4140\n",
      "Epoch 04 | Train Loss (MSE): 113993.0711 | Val MAE: 205.8062\n",
      "Epoch 05 | Train Loss (MSE): 111934.9818 | Val MAE: 206.9547\n",
      "Epoch 06 | Train Loss (MSE): 110261.6691 | Val MAE: 205.7208\n",
      "Epoch 07 | Train Loss (MSE): 109695.6403 | Val MAE: 205.8992\n",
      "Epoch 08 | Train Loss (MSE): 108387.3674 | Val MAE: 205.4476\n",
      "Epoch 09 | Train Loss (MSE): 109790.4970 | Val MAE: 204.7956\n",
      "Epoch 10 | Train Loss (MSE): 107669.8063 | Val MAE: 204.3870\n",
      "Best Val MAE for Fold 4: 204.3870\n",
      "------------------------------\n",
      "--- FOLD 5/5 ---\n",
      "------------------------------\n",
      "  [Model Init] n_teams: 740\n",
      "  [Model Init] n_heroes: 133\n",
      "  [Model Init] n_sides: 3\n",
      "  [Model Init] n_positions: 6\n",
      "Epoch 01 | Train Loss (MSE): 601509.2650 | Val MAE: 214.3399\n",
      "Epoch 02 | Train Loss (MSE): 114484.5438 | Val MAE: 214.3035\n",
      "Epoch 03 | Train Loss (MSE): 112674.1121 | Val MAE: 212.1294\n",
      "Epoch 04 | Train Loss (MSE): 112654.1115 | Val MAE: 209.9416\n",
      "Epoch 05 | Train Loss (MSE): 107441.9757 | Val MAE: 211.2563\n",
      "Epoch 06 | Train Loss (MSE): 107957.2203 | Val MAE: 210.6083\n",
      "Epoch 07 | Train Loss (MSE): 106141.3759 | Val MAE: 213.9523\n",
      "Epoch 08 | Train Loss (MSE): 106302.4312 | Val MAE: 208.1526\n",
      "Epoch 09 | Train Loss (MSE): 104280.3226 | Val MAE: 210.7354\n",
      "Epoch 10 | Train Loss (MSE): 106068.4492 | Val MAE: 209.2367\n",
      "Best Val MAE for Fold 5: 208.1526\n",
      "\n",
      "==============================\n",
      "TOTAL OOF MAE: 207.27277\n",
      "TOTAL OOF RMSE: 283.42355\n",
      "==============================\n",
      "\n",
      "OOF duration predictions saved to a new Series 'oof_pred_series'.\n",
      "0     927.034851\n",
      "1     916.437927\n",
      "2     992.974792\n",
      "3    1044.515503\n",
      "4     980.845886\n",
      "Name: oof_pred_positional_model, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "print(\"--- Running Step 4: Starting K-Fold Training ---\")\n",
    "\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "oof_preds = np.zeros(len(df))\n",
    "oof_indices = [] # To ensure alignment\n",
    "groups = df['match_id']\n",
    "\n",
    "# --- 3. Start the K-Fold Loop ---\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups=groups)):\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # 1. Create DataLoaders\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "    train_dataset = MatchDataset(train_df, target_column=TARGET_COLUMN)\n",
    "    val_dataset = MatchDataset(val_df, target_column=TARGET_COLUMN)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = PositionalModel(n_teams, n_heroes, n_sides).to(DEVICE)\n",
    "    \n",
    "    # 3. Define Loss Function and Optimizer\n",
    "    criterion = nn.MSELoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    best_val_mae = float('inf') \n",
    "    best_model_state = None\n",
    "\n",
    "    # 4. Start Training Loop\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = train_loop(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_preds, val_targets = get_predictions(model, val_loader, DEVICE)\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:02d} | Train Loss (MSE): {train_loss:.4f} | Val MAE: {val_mae:.4f}\")\n",
    "        \n",
    "        if val_mae < best_val_mae:\n",
    "            best_val_mae = val_mae\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # 5. Get OOF predictions from the *best* model\n",
    "    print(f\"Best Val MAE for Fold {fold+1}: {best_val_mae:.4f}\")\n",
    "    model.load_state_dict(best_model_state) \n",
    "    oof_preds_fold, _ = get_predictions(model, val_loader, DEVICE)\n",
    "    \n",
    "    # 6. Store OOF predictions\n",
    "    oof_preds[val_idx] = oof_preds_fold\n",
    "    oof_indices.extend(val_idx)\n",
    "\n",
    "# --- 4. Final OOF Score and Saving to Series ---\n",
    "total_oof_targets = df[TARGET_COLUMN].iloc[oof_indices]\n",
    "total_oof_mae = mean_absolute_error(total_oof_targets, oof_preds[oof_indices])\n",
    "total_oof_rmse = np.sqrt(mean_squared_error(total_oof_targets, oof_preds[oof_indices]))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(f\"TOTAL OOF MAE: {total_oof_mae:.5f}\")\n",
    "print(f\"TOTAL OOF RMSE: {total_oof_rmse:.5f}\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create a new pandas Series for the OOF predictions\n",
    "oof_pred_series = pd.Series(\n",
    "    oof_preds,\n",
    "    index=df.index,\n",
    "    name='oof_pred_positional_model'\n",
    ")\n",
    "\n",
    "print(\"\\nOOF duration predictions saved to a new Series 'oof_pred_series'.\")\n",
    "print(oof_pred_series.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b9f860c-c82d-48ea-b936-d8f7d347d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embed_oof'] = oof_pred_series #generating the OOF preds, this time with the pick order taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22ce51bc-ea67-4ddb-9164-3a1b58943c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores (RMSE): [-276.06846585 -262.93091256 -268.60517461 -301.91729575 -271.55615534]\n",
      "Mean CV RMSE: -276.2156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_v3 = ['sum_90D', 'rf_pred', 'elo_diff', 'embed_oof']\n",
    "model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "# 'test' has the 'embed_oof' column from the cell above\n",
    "evaluate_model(model=model, features=features_v3, data=df, target='duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeee55c-be4f-4928-9bc0-e79269daa631",
   "metadata": {},
   "source": [
    "#### -278.6853 --> -276.2156\n",
    "\n",
    "Its not bad for the first iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf18e2-d191-4415-a72a-95f0487162af",
   "metadata": {},
   "source": [
    "All in all, there is obviously a lot of room for improvement. I think I can improve the model by engineering better features, especially, trying to model each teams \"speed factor\"\n",
    "\n",
    "\n",
    "\n",
    "Possible K-clustering could be used here. Ask me about it, I have several ways forward. Thank you :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a1788-404b-4a48-a612-0b947163b0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cb633-cb5b-4b0a-a6ab-b6a88faed801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0723b4d-468c-4161-9347-ba2c408f2fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Pip)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
